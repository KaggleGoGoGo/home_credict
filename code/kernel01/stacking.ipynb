{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "\n",
    "org_dic = {'Advertising': 2,\n",
    " 'Agriculture': 3,\n",
    " 'Bank': 0,\n",
    " 'Business Entity Type 1': 2,\n",
    " 'Business Entity Type 2': 2,\n",
    " 'Business Entity Type 3': 2,\n",
    " 'Cleaning': 3,\n",
    " 'Construction': 3,\n",
    " 'Culture': 0,\n",
    " 'Electricity': 1,\n",
    " 'Emergency': 1,\n",
    " 'Government': 1,\n",
    " 'Hotel': 1,\n",
    " 'Housing': 1,\n",
    " 'Industry: type 1': 3,\n",
    " 'Industry: type 10': 1,\n",
    " 'Industry: type 11': 2,\n",
    " 'Industry: type 12': 0,\n",
    " 'Industry: type 13': 3,\n",
    " 'Industry: type 2': 1,\n",
    " 'Industry: type 3': 3,\n",
    " 'Industry: type 4': 3,\n",
    " 'Industry: type 5': 1,\n",
    " 'Industry: type 6': 1,\n",
    " 'Industry: type 7': 2,\n",
    " 'Industry: type 8': 3,\n",
    " 'Industry: type 9': 1,\n",
    " 'Insurance': 0,\n",
    " 'Kindergarten': 1,\n",
    " 'Legal Services': 1,\n",
    " 'Medicine': 1,\n",
    " 'Military': 0,\n",
    " 'Mobile': 2,\n",
    " 'Other': 1,\n",
    " 'Police': 0,\n",
    " 'Postal': 2,\n",
    " 'Realtor': 3,\n",
    " 'Religion': 0,\n",
    " 'Restaurant': 3,\n",
    " 'School': 0,\n",
    " 'Security': 2,\n",
    " 'Security Ministries': 0,\n",
    " 'Self-employed': 3,\n",
    " 'Services': 1,\n",
    " 'Telecom': 1,\n",
    " 'Trade: type 1': 2,\n",
    " 'Trade: type 2': 1,\n",
    " 'Trade: type 3': 3,\n",
    " 'Trade: type 4': 0,\n",
    " 'Trade: type 5': 1,\n",
    " 'Trade: type 6': 0,\n",
    " 'Trade: type 7': 2,\n",
    " 'Transport: type 1': 0,\n",
    " 'Transport: type 2': 1,\n",
    " 'Transport: type 3': 3,\n",
    " 'Transport: type 4': 2,\n",
    " 'University': 0,\n",
    " 'XNA': 0}\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess application_train.csv and application_test.csv\n",
    "def application_train_test(num_rows = None, nan_as_category = False):\n",
    "    # Read data and merge\n",
    "    df = pd.read_csv('../../data/application_train.csv', nrows= num_rows)\n",
    "    test_df = pd.read_csv('../../data/application_test.csv', nrows= num_rows)\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
    "    df = df.append(test_df).reset_index()\n",
    "    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    \n",
    "    docs = [_f for _f in df.columns if 'FLAG_DOC' in _f]\n",
    "    live = [_f for _f in df.columns if ('FLAG_' in _f) & ('FLAG_DOC' not in _f) & ('_FLAG_' not in _f)]\n",
    "    # 我添加的\n",
    "    df['DAYS_EMPLOYED_365243'] = df['DAYS_EMPLOYED'].map(lambda x: 1 if x == 365243 else 0 if not pd.isnull(x) else np.nan)\n",
    "    #for col in Nan_matters_col:\n",
    "    #    df[col+'_NAN'] = df[col].map(lambda x: 1 if  pd.isnull(x) else 0)\n",
    "    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "    \n",
    "    inc_by_org = df[['AMT_INCOME_TOTAL', 'ORGANIZATION_TYPE']].groupby('ORGANIZATION_TYPE').median()['AMT_INCOME_TOTAL']\n",
    "\n",
    "    df['NEW_CREDIT_TO_ANNUITY_RATIO'] = (df['AMT_CREDIT'] / df['AMT_ANNUITY']).replace(np.inf, np.nan)\n",
    "    df['NEW_CREDIT_TO_GOODS_RATIO'] = (df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']).replace(np.inf, np.nan)\n",
    "    df['NEW_DOC_IND_KURT'] = df[docs].kurtosis(axis=1)\n",
    "    df['NEW_LIVE_IND_SUM'] = df[live].sum(axis=1)\n",
    "    df['NEW_INC_PER_CHLD'] = (df['AMT_INCOME_TOTAL'] / (1 + df['CNT_CHILDREN'])).replace(np.inf, np.nan)\n",
    "    df['NEW_INC_BY_ORG'] = df['ORGANIZATION_TYPE'].map(inc_by_org)\n",
    "    df['NEW_EMPLOY_TO_BIRTH_RATIO'] = (df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']).replace(np.inf, np.nan)\n",
    "    df['NEW_ANNUITY_TO_INCOME_RATIO'] = (df['AMT_ANNUITY'] / (1 + df['AMT_INCOME_TOTAL'])).replace(np.inf, np.nan)\n",
    "    df['NEW_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
    "    df['NEW_EXT_SOURCES_MEAN'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "    df['NEW_SCORES_STD'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n",
    "    df['NEW_SCORES_STD'] = df['NEW_SCORES_STD'].fillna(df['NEW_SCORES_STD'].mean())\n",
    "    df['NEW_CAR_TO_BIRTH_RATIO'] = (df['OWN_CAR_AGE'] / df['DAYS_BIRTH']).replace(np.inf, np.nan)\n",
    "    df['NEW_CAR_TO_EMPLOY_RATIO'] = (df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']).replace(np.inf, np.nan)\n",
    "    df['NEW_PHONE_TO_BIRTH_RATIO'] = (df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']).replace(np.inf, np.nan)\n",
    "    df['NEW_PHONE_TO_BIRTH_RATIO_EMPLOYER'] = (df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_EMPLOYED']).replace(np.inf, np.nan)\n",
    "    df['NEW_CREDIT_TO_INCOME_RATIO'] = (df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']).replace(np.inf, np.nan)\n",
    "    \n",
    "#     # 我的变量 start\n",
    "    df['have_car_and_house'] =  list(map(lambda x, y: 1 if x=='Y' and y=='Y' else 0, df['FLAG_OWN_CAR'], df['FLAG_OWN_REALTY']))\n",
    "    df['income_credict_ratio'] = (df['AMT_INCOME_TOTAL'] /df['AMT_CREDIT']).replace(np.inf, np.nan)\n",
    "    edu_map = {'Academic degree':0, 'Higher education':1, 'Incomplete higher':2, 'Secondary / secondary special':3, 'Lower secondary':4}\n",
    "    df['education_type'] = df['NAME_EDUCATION_TYPE'].map(edu_map)\n",
    "    df['credit_age_ratio'] = (df['AMT_CREDIT'] / df['DAYS_BIRTH']).replace(np.inf, np.nan)\n",
    "    df['days_start_work'] = (df['DAYS_BIRTH'] - df['DAYS_EMPLOYED'])\n",
    "    df['days_start_buy_car'] = (df['DAYS_BIRTH'] - df['OWN_CAR_AGE'])\n",
    "    df['mobile'] = list(map(int, np.logical_or(df['FLAG_EMP_PHONE'] , np.logical_or(df['FLAG_WORK_PHONE'], df['FLAG_PHONE']))))\n",
    "    df['adult_num'] = list(map(lambda x, y: 1 if x-y==1 and x != 1 else 0, df['CNT_FAM_MEMBERS'], df['CNT_CHILDREN']))\n",
    "    df['region_rationg'] = df['REGION_RATING_CLIENT'] * df['REGION_RATING_CLIENT_W_CITY']\n",
    "    credit_by_org = df[['AMT_CREDIT', 'ORGANIZATION_TYPE']].groupby('ORGANIZATION_TYPE').median()['AMT_CREDIT']\n",
    "    df['credict_by_org'] =  df['ORGANIZATION_TYPE'].map(credit_by_org)\n",
    "    df['org_is_nan'] = df['ORGANIZATION_TYPE'].map(lambda x: 1 if x == 'XNA' else 0)\n",
    "    df['org_id'] = df['ORGANIZATION_TYPE'].map(org_dic)\n",
    "    df['ext_source'] = np.nanmedian(df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']], axis=1)\n",
    "    df['ext_source2'] = np.nansum(df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']], axis=1)\n",
    "    df['ext_source3'] = np.nanmin(df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']], axis=1)\n",
    "\n",
    "\n",
    "    df['app missing'] = df.isnull().sum(axis = 1).values\n",
    "    df['app EXT_SOURCE_1 * EXT_SOURCE_2'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2']\n",
    "    df['app EXT_SOURCE_1 * EXT_SOURCE_3'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_3']\n",
    "    df['app EXT_SOURCE_2 * EXT_SOURCE_3'] = df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
    "    df['app EXT_SOURCE_1 * DAYS_EMPLOYED'] = df['EXT_SOURCE_1'] * df['DAYS_EMPLOYED']\n",
    "    df['app EXT_SOURCE_2 * DAYS_EMPLOYED'] = df['EXT_SOURCE_2'] * df['DAYS_EMPLOYED']\n",
    "    df['app EXT_SOURCE_3 * DAYS_EMPLOYED'] = df['EXT_SOURCE_3'] * df['DAYS_EMPLOYED']\n",
    "    df['app EXT_SOURCE_1 / DAYS_BIRTH'] = (df['EXT_SOURCE_1'] / df['DAYS_BIRTH']).replace(np.inf, np.nan)\n",
    "    df['app EXT_SOURCE_2 / DAYS_BIRTH'] = (df['EXT_SOURCE_2'] / df['DAYS_BIRTH']).replace(np.inf, np.nan)\n",
    "    df['app EXT_SOURCE_3 / DAYS_BIRTH'] = (df['EXT_SOURCE_3'] / df['DAYS_BIRTH']).replace(np.inf, np.nan)\n",
    "\n",
    "    df['app AMT_CREDIT - AMT_GOODS_PRICE'] = df['AMT_CREDIT'] - df['AMT_GOODS_PRICE']\n",
    "    df['app AMT_CREDIT / AMT_GOODS_PRICE'] = (df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']).replace(np.inf, np.nan)\n",
    "    df['app AMT_CREDIT / AMT_ANNUITY'] = (df['AMT_CREDIT'] / df['AMT_ANNUITY']).replace(np.inf, np.nan)\n",
    "    df['app AMT_CREDIT / AMT_INCOME_TOTAL'] = (df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']).replace(np.inf, np.nan)\n",
    "\n",
    "    df['app AMT_INCOME_TOTAL / 12 - AMT_ANNUITY'] = df['AMT_INCOME_TOTAL'] / 12. - df['AMT_ANNUITY']\n",
    "    df['app AMT_INCOME_TOTAL / AMT_ANNUITY'] = (df['AMT_INCOME_TOTAL'] / df['AMT_ANNUITY']).replace(np.inf, np.nan)\n",
    "    df['app AMT_INCOME_TOTAL - AMT_GOODS_PRICE'] = df['AMT_INCOME_TOTAL'] - df['AMT_GOODS_PRICE']\n",
    "    df['app AMT_INCOME_TOTAL / CNT_FAM_MEMBERS'] = (df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']).replace(np.inf, np.nan)\n",
    "    df['app AMT_INCOME_TOTAL / CNT_CHILDREN'] = (df['AMT_INCOME_TOTAL'] / (1 + df['CNT_CHILDREN'])).replace(np.inf, np.nan)\n",
    "\n",
    "    df['app most popular AMT_GOODS_PRICE'] = df['AMT_GOODS_PRICE'] \\\n",
    "                        .isin([225000, 450000, 675000, 900000]).map({True: 1, False: 0})\n",
    "    df['app popular AMT_GOODS_PRICE'] = df['AMT_GOODS_PRICE'] \\\n",
    "                        .isin([1125000, 1350000, 1575000, 1800000, 2250000]).map({True: 1, False: 0})\n",
    "\n",
    "    df['app OWN_CAR_AGE / DAYS_BIRTH'] = (df['OWN_CAR_AGE'] / df['DAYS_BIRTH']).replace(np.inf, np.nan)\n",
    "    df['app OWN_CAR_AGE / DAYS_EMPLOYED'] = (df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']).replace(np.inf, np.nan)\n",
    "\n",
    "    df['app DAYS_LAST_PHONE_CHANGE / DAYS_BIRTH'] = (df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']).replace(np.inf, np.nan)\n",
    "    df['app DAYS_LAST_PHONE_CHANGE / DAYS_EMPLOYED'] = (df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_EMPLOYED']).replace(np.inf, np.nan)\n",
    "    df['app DAYS_EMPLOYED - DAYS_BIRTH'] = (df['DAYS_EMPLOYED'] - df['DAYS_BIRTH']).replace(np.inf, np.nan)\n",
    "    df['app DAYS_EMPLOYED / DAYS_BIRTH'] = (df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']).replace(np.inf, np.nan)\n",
    "\n",
    "    df['app CNT_CHILDREN / CNT_FAM_MEMBERS'] = (df['CNT_CHILDREN'] / df['CNT_FAM_MEMBERS']).replace(np.inf, np.nan)\n",
    "    \n",
    "    #  我的变量 end\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Categorical features with Binary encode (0 or 1; two categories)\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    # Categorical features with One-Hot encode\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "    dropcolum=['FLAG_DOCUMENT_2','FLAG_DOCUMENT_4','FLAG_DOCUMENT_7',\n",
    "    'FLAG_DOCUMENT_8','FLAG_DOCUMENT_9','FLAG_DOCUMENT_10', \n",
    "    'FLAG_DOCUMENT_11','FLAG_DOCUMENT_12','FLAG_DOCUMENT_13',\n",
    "    'FLAG_DOCUMENT_14','FLAG_DOCUMENT_15','FLAG_DOCUMENT_16',\n",
    "    'FLAG_DOCUMENT_17','FLAG_DOCUMENT_18','FLAG_DOCUMENT_19',\n",
    "    'FLAG_DOCUMENT_20','FLAG_DOCUMENT_21']\n",
    "    df= df.drop(dropcolum,axis=1)\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "# Preprocess bureau.csv and bureau_balance.csv\n",
    "def bureau_and_balance(num_rows = None, nan_as_category = True):\n",
    "    bureau = pd.read_csv('../../data/bureau.csv', nrows = num_rows)\n",
    "    bb = pd.read_csv('../../data/bureau_balance.csv', nrows = num_rows)\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "    \n",
    "\n",
    "    def weighted_mean(x, balance):\n",
    "        weight = np.sqrt(-1 / balance[x.index])\n",
    "        return np.nanmean(np.multiply(x, weight))\n",
    "    \n",
    "    bb['bb_not_zero'] = bb.loc[:, ['STATUS_1', 'STATUS_2', 'STATUS_3', 'STATUS_4', 'STATUS_5']].sum(axis=1)\n",
    "    # bb['bb_c_ratio'] =bb['STATUS_C'] / bb['BUREAU_BAL_COUNT']\n",
    "    \n",
    "    \n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size'],\n",
    "                      'bb_not_zero':['max', 'mean']}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "        \n",
    "    bb = pd.merge(bb, bureau[['SK_ID_BUREAU', 'SK_ID_CURR']], left_on='SK_ID_BUREAU', right_on='SK_ID_BUREAU', how='left')\n",
    "\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    \n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    temp = [i for i in bb_agg.columns if 'STATUS_' in i]\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    bureau.loc[bureau['AMT_ANNUITY'] > .8e8, 'AMT_ANNUITY'] = np.nan\n",
    "    bureau.loc[bureau['AMT_CREDIT_SUM'] > 3e8, 'AMT_CREDIT_SUM'] = np.nan\n",
    "    bureau.loc[bureau['AMT_CREDIT_SUM_DEBT'] > 1e8, 'AMT_CREDIT_SUM_DEBT'] = np.nan\n",
    "    bureau.loc[bureau['AMT_CREDIT_MAX_OVERDUE'] > .8e8, 'AMT_CREDIT_MAX_OVERDUE'] = np.nan\n",
    "    bureau.loc[bureau['DAYS_ENDDATE_FACT'] < -10000, 'DAYS_ENDDATE_FACT'] = np.nan\n",
    "    bureau.loc[(bureau['DAYS_CREDIT_UPDATE'] > 0) | (bureau['DAYS_CREDIT_UPDATE'] < -40000), 'DAYS_CREDIT_UPDATE'] = np.nan\n",
    "    bureau.loc[bureau['DAYS_CREDIT_ENDDATE'] < -10000, 'DAYS_CREDIT_ENDDATE'] = np.nan\n",
    "    \n",
    "    bureau['app_credit_annuity_ratio'] = (bureau['AMT_CREDIT_SUM'] /  bureau['AMT_ANNUITY']).replace(np.inf, np.nan)\n",
    "    bureau['app_credit_debt_ratio'] = (bureau['AMT_CREDIT_SUM'] /  bureau['AMT_CREDIT_SUM_DEBT']).replace(np.inf, np.nan)\n",
    "    bureau['app_credit_limit_ratio'] = (bureau['AMT_CREDIT_SUM'] /  bureau['AMT_CREDIT_SUM_LIMIT']).replace(np.inf, np.nan)\n",
    "    bureau['app_credit_overdue_ratio'] = (bureau['AMT_CREDIT_SUM'] /  bureau['AMT_CREDIT_SUM_OVERDUE']).replace(np.inf, np.nan)\n",
    "    bureau['weighted_credit'] = (bureau['AMT_CREDIT_SUM'] / np.sqrt(-bureau['DAYS_CREDIT'])).replace(np.inf, np.nan)\n",
    "    \n",
    "\n",
    "    bureau['bureau AMT_CREDIT_SUM - AMT_CREDIT_SUM_DEBT'] = bureau['AMT_CREDIT_SUM'] - bureau['AMT_CREDIT_SUM_DEBT']\n",
    "    bureau['bureau AMT_CREDIT_SUM - AMT_CREDIT_SUM_LIMIT'] = bureau['AMT_CREDIT_SUM'] - bureau['AMT_CREDIT_SUM_LIMIT']\n",
    "    bureau['bureau AMT_CREDIT_SUM - AMT_CREDIT_SUM_OVERDUE'] = bureau['AMT_CREDIT_SUM'] - bureau['AMT_CREDIT_SUM_OVERDUE']\n",
    "    \n",
    "    \n",
    "    bureau['bureau DAYS_CREDIT - CREDIT_DAY_OVERDUE'] = bureau['DAYS_CREDIT'] - bureau['CREDIT_DAY_OVERDUE']\n",
    "    bureau['bureau DAYS_CREDIT - DAYS_CREDIT_ENDDATE'] = bureau['DAYS_CREDIT'] - bureau['DAYS_CREDIT_ENDDATE']\n",
    "    bureau['bureau DAYS_CREDIT - DAYS_ENDDATE_FACT'] = bureau['DAYS_CREDIT'] - bureau['DAYS_ENDDATE_FACT']\n",
    "    bureau['bureau DAYS_CREDIT_ENDDATE - DAYS_ENDDATE_FACT'] = bureau['DAYS_CREDIT_ENDDATE'] - bureau['DAYS_ENDDATE_FACT']\n",
    "    bureau['bureau DAYS_CREDIT_UPDATE - DAYS_CREDIT_ENDDATE'] = bureau['DAYS_CREDIT_UPDATE'] - bureau['DAYS_CREDIT_ENDDATE']\n",
    "    \n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': [ 'mean', 'var', 'min'],\n",
    "        'DAYS_CREDIT_ENDDATE': [ 'mean', 'max'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean', 'min'],\n",
    "        'DAYS_ENDDATE_FACT':['mean', 'max', 'min', 'var'],\n",
    "        'CREDIT_DAY_OVERDUE': ['mean', 'max', 'var'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': [ 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': [ 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean','median'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum','mean','max'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum'],\n",
    "        'app_credit_annuity_ratio':['mean', 'median'],\n",
    "        'app_credit_debt_ratio':['mean', 'median'],\n",
    "        'app_credit_limit_ratio':['mean', 'median'],\n",
    "        'app_credit_overdue_ratio':['mean', 'median'],\n",
    "        'weighted_credit':['mean', 'sum', 'median'],\n",
    "        'bb_not_zero_MEAN':['mean'],\n",
    "        'bb_not_zero_MAX':['mean'],\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for col in bureau.columns:\n",
    "        if 'bureau' in col:\n",
    "            cat_aggregations[col] = ['mean', 'max', 'min', 'std', 'median']\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean', 'sum']\n",
    "    for cat in temp:\n",
    "        cat_aggregations[cat] = ['mean', 'std', 'median']\n",
    "    #for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean', 'sum', mode]\n",
    "    \n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "    return bureau_agg\n",
    "\n",
    "\n",
    "# Preprocess previous_applications.csv\n",
    "def previous_applications(num_rows = None, nan_as_category = True):\n",
    "    prev = pd.read_csv('../../data/previous_application.csv', nrows = num_rows)\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category= True)\n",
    "    \n",
    "    #prev['DAYS_LAST_DUE_ISNAN'] = prev['DAYS_LAST_DUE'].map(lambda x: 1 if x == 365243 else 0 if not pd.isnull(x) else np.nan)\n",
    "    #prev['DAYS_FIRST_DRAWING_ISNAN'] = prev['DAYS_FIRST_DRAWING'].map(lambda x: 1 if x == 365243 else 0 if not pd.isnull(x) else np.nan)\n",
    "    #prev['DAYS_FIRST_DUE_ISNAN'] = prev['DAYS_FIRST_DUE'].map(lambda x: 1 if x == 365243 else 0 if not pd.isnull(x) else np.nan)\n",
    "    #prev['DAYS_LAST_DUE_1ST_VERSION_ISNAN'] = prev['DAYS_LAST_DUE_1ST_VERSION'].map(lambda x: 1 if x == 365243 else 0 if not pd.isnull(x) else np.nan)\n",
    "    #prev['DAYS_TERMINATION_ISNAN'] = prev['DAYS_TERMINATION'].map(lambda x: 1 if x == 365243 else 0 if not pd.isnull(x) else np.nan)\n",
    "    # Days 365.243 values -> nan\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "    \n",
    "    # my features start\n",
    "    prev['prev missing'] = prev.isnull().sum(axis = 1).values\n",
    "    prev['prev AMT_APPLICATION / AMT_CREDIT'] = (prev['AMT_APPLICATION'] / prev['AMT_CREDIT']).replace(np.inf, np.nan)\n",
    "    prev['prev AMT_APPLICATION - AMT_CREDIT'] = prev['AMT_APPLICATION'] - prev['AMT_CREDIT']\n",
    "    prev['prev AMT_APPLICATION - AMT_GOODS_PRICE'] = prev['AMT_APPLICATION'] - prev['AMT_GOODS_PRICE']\n",
    "    prev['prev AMT_GOODS_PRICE - AMT_CREDIT'] = prev['AMT_GOODS_PRICE'] - prev['AMT_CREDIT']\n",
    "    prev['prev DAYS_FIRST_DRAWING - DAYS_FIRST_DUE'] = prev['DAYS_FIRST_DRAWING'] - prev['DAYS_FIRST_DUE']\n",
    "    prev['prev DAYS_TERMINATION less -500'] = (prev['DAYS_TERMINATION'] < -500).astype(int)\n",
    "    \n",
    "    prev['prev AMT_CREDIT / AMT_ANNUITY'] = (prev['AMT_CREDIT'] / prev['AMT_ANNUITY']).replace(np.inf, np.nan)\n",
    "    prev['prev AMT_APPLICATION / AMT_ANNUITY'] = (prev['AMT_APPLICATION'] / prev['AMT_ANNUITY']).replace(np.inf, np.nan)\n",
    "    prev['prev AMT_APPLICATION - AMT_ANNUITY'] = prev['AMT_APPLICATION'] - prev['AMT_ANNUITY']\n",
    "    prev['prev AMT_APPLICATION / AMT_DOWN_PAYMENT'] = (prev['AMT_APPLICATION'] / prev['AMT_DOWN_PAYMENT']).replace(np.inf, np.nan)\n",
    "    prev['prev AMT_APPLICATION - AMT_DOWN_PAYMENT'] = prev['AMT_APPLICATION'] - prev['AMT_DOWN_PAYMENT']\n",
    "    prev['prev RATE_INTEREST_PRIMARY - RATE_INTEREST_PRIVILEGED'] = prev['RATE_INTEREST_PRIMARY'] - prev['RATE_INTEREST_PRIVILEGED']\n",
    "    prev['prev AMT_APPLICATION / CNT_PAYMENT'] = (prev['AMT_APPLICATION'] / prev['CNT_PAYMENT']).replace(np.inf, np.nan)\n",
    "    \n",
    "    \n",
    "    # my features end\n",
    "    \n",
    "    \n",
    "    # Add feature: value ask / value received percentage\n",
    "    #prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': [ 'max', 'mean', 'min'],\n",
    "        'AMT_APPLICATION': [ 'max','mean', 'min', 'size'],\n",
    "        'AMT_CREDIT': [ 'max', 'mean', 'min', 'std'],\n",
    "        #'APP_CREDIT_PERC': [ 'max', 'mean'],\n",
    "        'AMT_DOWN_PAYMENT': [ 'max', 'mean', 'min'],\n",
    "        'AMT_GOODS_PRICE': [ 'max', 'mean', 'min', 'sum'],\n",
    "        'HOUR_APPR_PROCESS_START': [ 'max', 'mean', 'min'],\n",
    "        'RATE_DOWN_PAYMENT': [ 'max', 'mean', 'min'],\n",
    "        'DAYS_DECISION': [ 'max', 'mean', 'min'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "        'RATE_INTEREST_PRIMARY': [ 'max', 'mean', 'min'],\n",
    "        'RATE_INTEREST_PRIVILEGED': [ 'max', 'mean', 'min'],\n",
    "    }\n",
    "#         'DAYS_LAST_DUE_ISNAN':['mean', 'sum'], \n",
    "#         'DAYS_FIRST_DRAWING_ISNAN':['mean', 'sum'], \n",
    "#         'DAYS_FIRST_DUE_ISNAN':['mean', 'sum'], \n",
    "#         'DAYS_LAST_DUE_1ST_VERSION_ISNAN':['mean', 'sum'], \n",
    "#         'DAYS_TERMINATION_ISNAN':['mean', 'sum'], \n",
    "    for i in prev.columns:\n",
    "        if 'prev' in i:\n",
    "            num_aggregations[i] = ['mean'] #  'min', 'max', 'size', 'mean', 'var', 'sum']\n",
    "    \n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "    \n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    gc.collect()\n",
    "    return prev_agg\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess POS_CASH_balance.csv\n",
    "def pos_cash(num_rows = None, nan_as_category = True):\n",
    "    pos = pd.read_csv('../../data/POS_CASH_balance.csv', nrows = num_rows)\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\n",
    "    \n",
    "    # my feature start\n",
    "    pos['pos installment_sum'] = pos['CNT_INSTALMENT'] + pos['CNT_INSTALMENT_FUTURE']\n",
    "    pos['pos dpd_sum'] = pos['SK_DPD'] + pos['SK_DPD_DEF']\n",
    "    \n",
    "    # my feature end\n",
    "    \n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size', 'min'],\n",
    "        'SK_DPD': [ 'max', 'mean'],\n",
    "        'SK_DPD_DEF': [ 'max', 'mean'],\n",
    "        'pos installment_sum': ['sum', 'max', 'mean'],\n",
    "        'CNT_INSTALMENT': ['sum', 'max', 'mean'],\n",
    "        'CNT_INSTALMENT_FUTURE': ['sum', 'max', 'mean'],\n",
    "        'pos dpd_sum': ['sum', 'max', 'mean'],\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] =[ 'max', 'mean', 'sum']\n",
    "    \n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    \n",
    "    pos_agg_prev = pos.groupby('SK_ID_PREV').agg(aggregations)\n",
    "    pos_agg_prev.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg_prev.columns.tolist()])\n",
    "    pos_agg_prev = pd.merge(pos_agg_prev, pos[['SK_ID_PREV', 'SK_ID_CURR']], left_index=True, right_on='SK_ID_PREV',how='left')\n",
    "    pos_agg_prev = pos_agg_prev.drop(columns=['SK_ID_PREV'])\n",
    "    aggregation2 = {}\n",
    "    for i in pos_agg_prev.columns:\n",
    "        aggregation2[i] = ['mean', 'max']\n",
    "    pos_agg_prev2 = pos_agg_prev.groupby('SK_ID_CURR').agg(aggregation2)\n",
    "    pos_agg_prev2.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg_prev2.columns.tolist()])\n",
    "    pos_agg_prev2 = pos_agg_prev2.drop(columns=['POS_SK_ID_CURR_MEAN', 'POS_SK_ID_CURR_MAX'])\n",
    "    \n",
    "    pos_agg = pos_agg.join(pos_agg_prev2, how='left')\n",
    "    # Count pos cash accounts\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "    \n",
    "    del pos, pos_agg_prev, pos_agg_prev2\n",
    "    gc.collect()\n",
    "    return pos_agg\n",
    "\n",
    "\n",
    "# Preprocess installments_payments.csv\n",
    "def installments_payments(num_rows = None, nan_as_category = True):\n",
    "    ins = pd.read_csv('../../data/installments_payments.csv', nrows = num_rows)\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n",
    "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "    ins['PAYMENT_PERC'] = (ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']).replace(np.inf, np.nan)\n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    \n",
    "    # my feature start\n",
    "    ins['ins AMT_INSTALMENT==AMT_PAYMENT'] = (ins['AMT_INSTALMENT'] == ins['AMT_PAYMENT']).astype('int')\n",
    "    ins['ins DPD_ratio'] = (ins['DAYS_ENTRY_PAYMENT'] / ins['DAYS_INSTALMENT']).replace(np.inf, np.nan)\n",
    "    ins['ins NUM_INSTALMENT_VERSION=1'] = ins['NUM_INSTALMENT_VERSION'].map(lambda x: 1 if x == 1 else 0 if not pd.isnull(x) else np.nan)\n",
    "    ins['ins NUM_INSTALMENT_VERSION=0'] = ins['NUM_INSTALMENT_VERSION'].map(lambda x: 1 if x == 0 else 0 if not pd.isnull(x) else np.nan)\n",
    "    #ins['ins AMT_PAYMENT / DPD'] =(ins['AMT_PAYMENT'] / ins['DPD']).replace(np.inf, np.nan)\n",
    "    \n",
    "    # my feature end\n",
    "    \n",
    "    \n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum','min' ],\n",
    "        'DBD': ['max', 'mean', 'sum','min'],\n",
    "        'PAYMENT_PERC': [ 'max','mean','min'],\n",
    "        'PAYMENT_DIFF': [ 'max','mean','min'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum','min'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum'],\n",
    "        'NUM_INSTALMENT_NUMBER': ['max'],\n",
    "        'NUM_INSTALMENT_VERSION': ['max'],\n",
    "    }\n",
    "    for col in ins.columns:\n",
    "        if 'ins' in col:\n",
    "            aggregations[col] = ['mean', 'max', 'min']\n",
    "    \n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] =[ 'max', 'mean', 'sum']\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    \n",
    "    \n",
    "    ins_agg_prev = ins.groupby('SK_ID_PREV').agg(aggregations)\n",
    "    ins_agg_prev.columns = pd.Index(['INS_' + e[0] + \"_\" + e[1].upper() for e in ins_agg_prev.columns.tolist()])\n",
    "    ins_agg_prev = pd.merge(ins_agg_prev, ins[['SK_ID_PREV', 'SK_ID_CURR']], left_index=True, right_on='SK_ID_PREV',how='left')\n",
    "    ins_agg_prev = ins_agg_prev.drop(columns=['SK_ID_PREV'])\n",
    "    aggregation2 = {}\n",
    "    for i in ins_agg_prev.columns:\n",
    "        aggregation2[i] = ['mean', 'max', 'min']\n",
    "    ins_agg_prev2 = ins_agg_prev.groupby('SK_ID_CURR').agg(aggregation2)\n",
    "    ins_agg_prev2.columns = pd.Index(['INS_' + e[0] + \"_\" + e[1].upper() for e in ins_agg_prev2.columns.tolist()])\n",
    "    ins_agg_prev2 = ins_agg_prev2.drop(columns=['INS_SK_ID_CURR_MEAN', 'INS_SK_ID_CURR_MAX','INS_SK_ID_CURR_MIN'])\n",
    "    \n",
    "    ins_agg = ins_agg.join(ins_agg_prev2, how='left')\n",
    "    \n",
    "    \n",
    "    # Count installments accounts\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "    del ins\n",
    "    gc.collect()\n",
    "    return ins_agg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess credit_card_balance.csv\n",
    "def credit_card_balance(num_rows = None, nan_as_category = True):\n",
    "    cc = pd.read_csv('../../data/credit_card_balance.csv', nrows = num_rows)\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n",
    "    \n",
    "    \n",
    "    # my feature start\n",
    "    cc['cc AMT_BALANCE / MONTHS_BALANCE'] = (cc['AMT_BALANCE'] / cc['MONTHS_BALANCE']).replace(np.inf, np.nan)\n",
    "    cc['cc AMT_CREDIT_LIMIT_ACTUAL / AMT_BALANCE'] = (cc['AMT_CREDIT_LIMIT_ACTUAL'] / cc['AMT_BALANCE']).replace(np.inf, np.nan)\n",
    "    cc['cc AMOUNT_DRAWING_SUM'] = cc['AMT_DRAWINGS_ATM_CURRENT'] + cc['AMT_DRAWINGS_CURRENT'] + \\\n",
    "                            cc['AMT_DRAWINGS_OTHER_CURRENT'] + cc['AMT_DRAWINGS_POS_CURRENT']\n",
    "    cc['cc AMT_BALANCE / AMT_INST_MIN_REGULARITY'] = (cc['AMT_BALANCE'] / cc['AMT_INST_MIN_REGULARITY']).replace(np.inf, np.nan)\n",
    "    cc['cc AMT_BALANCE / AMT_PAYMENT_CURRENT'] = (cc['AMT_BALANCE'] / cc['AMT_PAYMENT_CURRENT']).replace(np.inf, np.nan)\n",
    "    cc['cc AMT_PAYMENT_CURRENT / AMT_INST_MIN_REGULARITY'] = (cc['AMT_PAYMENT_CURRENT'] / cc['AMT_INST_MIN_REGULARITY']).replace(np.inf, np.nan)\n",
    "    cc['cc AMT_PAYMENT_CURRENT / AMT_PAYMENT_TOTAL_CURRENT'] = (cc['AMT_PAYMENT_CURRENT'] / cc['AMT_PAYMENT_TOTAL_CURRENT']).replace(np.inf, np.nan)\n",
    "    cc['cc AMT_RECIVABLE / AMT_RECEIVABLE_PRINCIPAL'] = (cc['AMT_RECIVABLE'] / cc['AMT_RECEIVABLE_PRINCIPAL']).replace(np.inf, np.nan)\n",
    "    cc['cc AMT_RECIVABLE / AMT_TOTAL_RECEIVABLE'] = (cc['AMT_RECIVABLE'] / cc['AMT_TOTAL_RECEIVABLE']).replace(np.inf, np.nan)\n",
    "    \n",
    "    \n",
    "    cc['cc AMT_DRAWINGS_ATM_CURRENT / CNT_DRAWINGS_ATM_CURRENT'] = (cc['AMT_DRAWINGS_ATM_CURRENT'] / cc['CNT_DRAWINGS_ATM_CURRENT']).replace(np.inf, np.nan)\n",
    "    cc['cc AMT_DRAWINGS_CURRENT / CNT_DRAWINGS_CURRENT'] = (cc['AMT_DRAWINGS_CURRENT'] / cc['CNT_DRAWINGS_CURRENT']).replace(np.inf, np.nan)\n",
    "    cc['cc AMT_DRAWINGS_OTHER_CURRENT / CNT_DRAWINGS_OTHER_CURRENT'] = (cc['AMT_DRAWINGS_OTHER_CURRENT'] / cc['CNT_DRAWINGS_OTHER_CURRENT']).replace(np.inf, np.nan)\n",
    "    cc['cc AMT_DRAWINGS_POS_CURRENT / CNT_DRAWINGS_POS_CURRENT'] = (cc['AMT_DRAWINGS_POS_CURRENT'] / cc['CNT_DRAWINGS_POS_CURRENT']).replace(np.inf, np.nan)\n",
    "    \n",
    "    cc['cc CNT_DRAWING_SUM'] = cc['CNT_DRAWINGS_ATM_CURRENT'] + cc['CNT_DRAWINGS_CURRENT'] + \\\n",
    "                            cc['CNT_DRAWINGS_OTHER_CURRENT'] + cc['CNT_DRAWINGS_POS_CURRENT']\n",
    "    \n",
    "    cc['cc AMT_DRAWING / CNT_DRAWING'] = (cc['cc AMOUNT_DRAWING_SUM']  / cc['cc CNT_DRAWING_SUM']).replace(np.inf, np.nan)\n",
    "    # my feature end\n",
    "   \n",
    "    cc_agg_prev = cc.groupby('SK_ID_PREV').agg([ 'max', 'mean'])\n",
    "    cc_agg_prev.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg_prev.columns.tolist()])\n",
    "    cc_agg_prev = pd.merge(cc_agg_prev, cc[['SK_ID_PREV', 'SK_ID_CURR']], left_index=True, right_on='SK_ID_PREV',how='left')\n",
    "    cc_agg_prev = cc_agg_prev.drop(columns=['SK_ID_PREV'])\n",
    "    cc_agg_prev2 = cc_agg_prev.groupby('SK_ID_CURR').agg(['mean'])\n",
    "    cc_agg_prev2.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg_prev2.columns.tolist()])\n",
    "    #cc_agg_prev2 = cc_agg_prev2.drop(columns=['CC_SK_ID_CURR_MEAN', 'CC_SK_ID_CURR_MAX','CC_SK_ID_CURR_MIN'])\n",
    "    \n",
    "    \n",
    "    # General aggregations\n",
    "    cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg([ 'max', 'mean', 'min', 'sum'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    \n",
    "    cc_agg = cc_agg.join(cc_agg_prev2, how='left')\n",
    "    \n",
    "    \n",
    "    # Count credit card lines\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "    del cc\n",
    "    gc.collect()\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 307511, test samples: 48744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:4033: RuntimeWarning: All-NaN slice encountered\n",
      "  r = func(a, **kwargs)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:146: RuntimeWarning: All-NaN axis encountered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bureau df shape: (305811, 254)\n",
      "Process bureau and bureau_balance - done in 41s\n",
      "Previous applications df shape: (338857, 306)\n",
      "Process previous_applications - done in 30s\n",
      "Pos-cash balance df shape: (337252, 151)\n",
      "Process POS-CASH balance - done in 69s\n",
      "Installments payments df shape: (339587, 157)\n",
      "Process installments payments - done in 87s\n",
      "Credit card balance df shape: (103558, 261)\n",
      "Process credit card balance - done in 42s\n"
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "num_rows = 10000 if debug else None\n",
    "df = application_train_test(num_rows)\n",
    "with timer(\"Process bureau and bureau_balance\"):\n",
    "    bureau = bureau_and_balance(num_rows)\n",
    "    print(\"Bureau df shape:\", bureau.shape)\n",
    "    df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
    "    del bureau\n",
    "    gc.collect()\n",
    "with timer(\"Process previous_applications\"):\n",
    "    prev = previous_applications(num_rows)\n",
    "    print(\"Previous applications df shape:\", prev.shape)\n",
    "    df = df.join(prev, how='left', on='SK_ID_CURR')\n",
    "    del prev\n",
    "    gc.collect()\n",
    "with timer(\"Process POS-CASH balance\"):\n",
    "    pos = pos_cash(num_rows)\n",
    "    print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "    df = df.join(pos, how='left', on='SK_ID_CURR')\n",
    "    del pos\n",
    "    gc.collect()\n",
    "with timer(\"Process installments payments\"):\n",
    "    ins = installments_payments(num_rows)\n",
    "    print(\"Installments payments df shape:\", ins.shape)\n",
    "    df = df.join(ins, how='left', on='SK_ID_CURR')\n",
    "    del ins\n",
    "    gc.collect()\n",
    "with timer(\"Process credit card balance\"):\n",
    "    cc = credit_card_balance(num_rows)\n",
    "    print(\"Credit card balance df shape:\", cc.shape)\n",
    "    df = df.join(cc, how='left', on='SK_ID_CURR')\n",
    "    del cc\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "import xgboost as xgb\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from math import sqrt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "NFOLDS = 3\n",
    "SEED = 47\n",
    "\n",
    "\n",
    "x_train = df[df['TARGET'].notnull()]\n",
    "x_test = df[df['TARGET'].isnull()]\n",
    "y_train = x_train['TARGET']\n",
    "y_train = y_train.astype('int')\n",
    "ntrain = x_train.shape[0]\n",
    "ntest = x_test.shape[0]\n",
    "\n",
    "feats = [f for f in x_train.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "x_train = x_train[feats]\n",
    "x_test = x_test[feats]\n",
    "\n",
    "\n",
    "kf = KFold(n_splits = NFOLDS, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking之前检查其他模型效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.7,\n",
    "    'learning_rate': 0.075,\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 4,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 1,\n",
    "    'nrounds': 200\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.765579\tvalidation_1-auc:0.731246\n",
      "Multiple eval metrics have been passed: 'validation_1-auc' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-auc hasn't improved in 200 rounds.\n",
      "[99]\tvalidation_0-auc:0.819448\tvalidation_1-auc:0.765204\n",
      "[0]\tvalidation_0-auc:0.764799\tvalidation_1-auc:0.725526\n",
      "Multiple eval metrics have been passed: 'validation_1-auc' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-auc hasn't improved in 200 rounds.\n",
      "[99]\tvalidation_0-auc:0.820376\tvalidation_1-auc:0.763794\n",
      "[0]\tvalidation_0-auc:0.763297\tvalidation_1-auc:0.72877\n",
      "Multiple eval metrics have been passed: 'validation_1-auc' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-auc hasn't improved in 200 rounds.\n",
      "[99]\tvalidation_0-auc:0.819455\tvalidation_1-auc:0.764047\n",
      "[0]\tvalidation_0-auc:0.764551\tvalidation_1-auc:0.728797\n",
      "Multiple eval metrics have been passed: 'validation_1-auc' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-auc hasn't improved in 200 rounds.\n",
      "[99]\tvalidation_0-auc:0.820542\tvalidation_1-auc:0.765753\n",
      "[0]\tvalidation_0-auc:0.765098\tvalidation_1-auc:0.728006\n",
      "Multiple eval metrics have been passed: 'validation_1-auc' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-auc hasn't improved in 200 rounds.\n",
      "[99]\tvalidation_0-auc:0.820339\tvalidation_1-auc:0.760309\n",
      "0.7637324849269812\n"
     ]
    }
   ],
   "source": [
    "num_folds = 5\n",
    "folds = KFold(n_splits= num_folds, shuffle=True, random_state=47)\n",
    "# Create arrays and dataframes to store results\n",
    "oof_preds = np.zeros(x_train.shape[0])\n",
    "sub_preds = np.zeros(x_test.shape[0])\n",
    "# feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(x_train, y_train)):\n",
    "    train_x, train_y = x_train.iloc[train_idx], y_train.iloc[train_idx]\n",
    "    valid_x, valid_y = x_train.iloc[valid_idx], y_train.iloc[valid_idx]\n",
    "    # LightGBM parameters found by Bayesian optimization\n",
    "    clf = xgb.XGBClassifier(**xgb_params)\n",
    "\n",
    "    clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "            eval_metric= 'auc', verbose= 1000, early_stopping_rounds= 200)\n",
    "\n",
    "    oof_preds[valid_idx] = clf.predict_proba(valid_x, ntree_limit=clf.best_iteration)[:, 1]\n",
    "    sub_preds += clf.predict_proba(x_test, ntree_limit=clf.best_iteration)[:, 1] / folds.n_splits\n",
    "\n",
    "    del clf, train_x, train_y, valid_x, valid_y\n",
    "    gc.collect()\n",
    "\n",
    "print(roc_auc_score(y_train, oof_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7783225909915263\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(y_train, oof_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CATBOOST\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_params = {\n",
    "    'iterations': 5000, #200,\n",
    "    'learning_rate': 0.01,\n",
    "    'depth': 8, #3,\n",
    "    'l2_leaf_reg': 40,\n",
    "    'bootstrap_type': 'Bernoulli',\n",
    "    'subsample': 0.7,\n",
    "    'scale_pos_weight': 5,\n",
    "    'eval_metric': 'AUC',\n",
    "    'od_type': 'Iter',\n",
    "    'allow_writing_files': False\n",
    "}\n",
    "clf = CatBoostClassifier(**catboost_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 330: There are NaNs in test dataset (feature number 25) but there were no NaNs in learn dataset\n",
    "\n",
    "def trans(train, test1, test2):\n",
    "    feat = []\n",
    "    for col in train.columns:\n",
    "        s1 = train[col].isnull().sum()\n",
    "        s2 = test1[col].isnull().sum()\n",
    "        s3 = test2[col].isnull().sum()\n",
    "        \n",
    "        if (s1 == 0 and s2 != 0) or (s1 == 0 and s3 != 0):\n",
    "            pass\n",
    "        else:\n",
    "            feat.append(col)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.7345010\tbest: 0.7345010 (0)\ttotal: 413ms\tremaining: 34m 24s\n",
      "1000:\ttest: 0.7904045\tbest: 0.7904045 (1000)\ttotal: 6m 42s\tremaining: 26m 47s\n",
      "2000:\ttest: 0.7950853\tbest: 0.7950853 (2000)\ttotal: 13m 17s\tremaining: 19m 55s\n",
      "3000:\ttest: 0.7969251\tbest: 0.7969299 (2998)\ttotal: 19m 57s\tremaining: 13m 17s\n",
      "4000:\ttest: 0.7978521\tbest: 0.7978521 (4000)\ttotal: 26m 37s\tremaining: 6m 38s\n",
      "4999:\ttest: 0.7982648\tbest: 0.7982710 (4996)\ttotal: 33m 15s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7982710329\n",
      "bestIteration = 4996\n",
      "\n",
      "Shrink model to first 4997 iterations.\n",
      "0:\ttest: 0.7302892\tbest: 0.7302892 (0)\ttotal: 409ms\tremaining: 34m 5s\n",
      "1000:\ttest: 0.7887256\tbest: 0.7887256 (1000)\ttotal: 6m 41s\tremaining: 26m 42s\n",
      "2000:\ttest: 0.7937143\tbest: 0.7937143 (2000)\ttotal: 13m 21s\tremaining: 20m\n",
      "3000:\ttest: 0.7956682\tbest: 0.7956682 (3000)\ttotal: 20m\tremaining: 13m 19s\n",
      "4000:\ttest: 0.7965413\tbest: 0.7965509 (3986)\ttotal: 26m 35s\tremaining: 6m 38s\n",
      "4999:\ttest: 0.7972345\tbest: 0.7972442 (4998)\ttotal: 33m 16s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7972441835\n",
      "bestIteration = 4998\n",
      "\n",
      "Shrink model to first 4999 iterations.\n",
      "0:\ttest: 0.7300987\tbest: 0.7300987 (0)\ttotal: 417ms\tremaining: 34m 44s\n",
      "1000:\ttest: 0.7888342\tbest: 0.7888342 (1000)\ttotal: 6m 40s\tremaining: 26m 41s\n",
      "2000:\ttest: 0.7931223\tbest: 0.7931223 (2000)\ttotal: 13m 10s\tremaining: 19m 44s\n",
      "3000:\ttest: 0.7949334\tbest: 0.7949334 (3000)\ttotal: 19m 46s\tremaining: 13m 10s\n",
      "4000:\ttest: 0.7957396\tbest: 0.7957446 (3999)\ttotal: 26m 22s\tremaining: 6m 35s\n",
      "Stopped by overfitting detector  (200 iterations wait)\n",
      "\n",
      "bestTest = 0.7957445741\n",
      "bestIteration = 3999\n",
      "\n",
      "Shrink model to first 4000 iterations.\n",
      "0:\ttest: 0.7322998\tbest: 0.7322998 (0)\ttotal: 417ms\tremaining: 34m 42s\n",
      "1000:\ttest: 0.7912182\tbest: 0.7912200 (999)\ttotal: 6m 44s\tremaining: 26m 57s\n",
      "2000:\ttest: 0.7952938\tbest: 0.7952966 (1998)\ttotal: 13m 36s\tremaining: 20m 24s\n",
      "3000:\ttest: 0.7967665\tbest: 0.7967717 (2996)\ttotal: 20m 30s\tremaining: 13m 39s\n",
      "4000:\ttest: 0.7975007\tbest: 0.7975025 (3992)\ttotal: 27m 22s\tremaining: 6m 50s\n",
      "4999:\ttest: 0.7978875\tbest: 0.7978986 (4982)\ttotal: 34m 6s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7978986209\n",
      "bestIteration = 4982\n",
      "\n",
      "Shrink model to first 4983 iterations.\n",
      "0:\ttest: 0.7274939\tbest: 0.7274939 (0)\ttotal: 405ms\tremaining: 33m 42s\n",
      "1000:\ttest: 0.7840583\tbest: 0.7840583 (1000)\ttotal: 6m 41s\tremaining: 26m 42s\n",
      "2000:\ttest: 0.7880330\tbest: 0.7880330 (2000)\ttotal: 13m 21s\tremaining: 20m 1s\n",
      "3000:\ttest: 0.7898610\tbest: 0.7898633 (2997)\ttotal: 20m 1s\tremaining: 13m 20s\n",
      "4000:\ttest: 0.7907129\tbest: 0.7907129 (4000)\ttotal: 26m 34s\tremaining: 6m 38s\n",
      "4999:\ttest: 0.7911397\tbest: 0.7911624 (4989)\ttotal: 33m 14s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7911623725\n",
      "bestIteration = 4989\n",
      "\n",
      "Shrink model to first 4990 iterations.\n",
      "0.7959798793334851\n"
     ]
    }
   ],
   "source": [
    "num_folds = 5\n",
    "folds = KFold(n_splits= num_folds, shuffle=True, random_state=47)\n",
    "# Create arrays and dataframes to store results\n",
    "oof_preds = np.zeros(x_train.shape[0])\n",
    "sub_preds = np.zeros(x_test.shape[0])\n",
    "# feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(x_train, y_train)):\n",
    "    test_x = x_test.copy()\n",
    "    train_x, train_y = x_train.iloc[train_idx], y_train.iloc[train_idx]\n",
    "    valid_x, valid_y = x_train.iloc[valid_idx], y_train.iloc[valid_idx]\n",
    "    # LightGBM parameters found by Bayesian optimization\n",
    "    clf = CatBoostClassifier(**catboost_params)\n",
    "    feat = trans(train_x, valid_x, test_x)\n",
    "    train_x = train_x[feat]\n",
    "    valid_x = valid_x[feat]\n",
    "    test_x = test_x[feat]\n",
    "    \n",
    "    clf.fit(train_x, train_y, eval_set=[(valid_x, valid_y)],  verbose= 1000, early_stopping_rounds= 200) #(train_x, train_y), \n",
    "\n",
    "    oof_preds[valid_idx] = clf.predict_proba(valid_x)[:, 1]\n",
    "    sub_preds += clf.predict_proba(test_x)[:, 1] / folds.n_splits \n",
    "\n",
    "    del clf, train_x, train_y, valid_x, valid_y\n",
    "    gc.collect()\n",
    "\n",
    "print(roc_auc_score(y_train, oof_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "x_test = df[df['TARGET'].isnull()]\n",
    "\n",
    "x_test['TARGET'] = sub_preds\n",
    "x_test[['SK_ID_CURR', 'TARGET']].to_csv(\"submission_kernel01_catboost.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.7046499\tbest: 0.7046499 (0)\ttotal: 188ms\tremaining: 37.3s\n",
      "199:\ttest: 0.7881009\tbest: 0.7886476 (138)\ttotal: 36.1s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7886476313\n",
      "bestIteration = 138\n",
      "\n",
      "Shrink model to first 139 iterations.\n",
      "0:\ttest: 0.6995302\tbest: 0.6995302 (0)\ttotal: 182ms\tremaining: 36.1s\n",
      "199:\ttest: 0.7899896\tbest: 0.7900347 (196)\ttotal: 34.7s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7900347288\n",
      "bestIteration = 196\n",
      "\n",
      "Shrink model to first 197 iterations.\n",
      "0:\ttest: 0.6987260\tbest: 0.6987260 (0)\ttotal: 199ms\tremaining: 39.5s\n",
      "199:\ttest: 0.7873359\tbest: 0.7877641 (136)\ttotal: 36.9s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7877641403\n",
      "bestIteration = 136\n",
      "\n",
      "Shrink model to first 137 iterations.\n",
      "0:\ttest: 0.7035180\tbest: 0.7035180 (0)\ttotal: 174ms\tremaining: 34.6s\n",
      "199:\ttest: 0.7880964\tbest: 0.7881091 (187)\ttotal: 34.8s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.788109143\n",
      "bestIteration = 187\n",
      "\n",
      "Shrink model to first 188 iterations.\n",
      "0:\ttest: 0.6969883\tbest: 0.6969883 (0)\ttotal: 196ms\tremaining: 38.9s\n",
      "199:\ttest: 0.7822784\tbest: 0.7827708 (144)\ttotal: 36.1s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7827708402\n",
      "bestIteration = 144\n",
      "\n",
      "Shrink model to first 145 iterations.\n",
      "0.787447818786379\n"
     ]
    }
   ],
   "source": [
    "num_folds = 5\n",
    "folds = KFold(n_splits= num_folds, shuffle=True, random_state=47)\n",
    "# Create arrays and dataframes to store results\n",
    "oof_preds = np.zeros(x_train.shape[0])\n",
    "sub_preds = np.zeros(x_test.shape[0])\n",
    "# feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(x_train, y_train)):\n",
    "    test_x = x_test.copy()\n",
    "    train_x, train_y = x_train.iloc[train_idx], y_train.iloc[train_idx]\n",
    "    valid_x, valid_y = x_train.iloc[valid_idx], y_train.iloc[valid_idx]\n",
    "    # LightGBM parameters found by Bayesian optimization\n",
    "    clf = CatBoostClassifier(**catboost_params)\n",
    "    feat = trans(train_x, valid_x, test_x)\n",
    "    train_x = train_x[feat]\n",
    "    valid_x = valid_x[feat]\n",
    "    test_x = test_x[feat]\n",
    "    \n",
    "    clf.fit(train_x, train_y, eval_set=[(valid_x, valid_y)],  verbose= 1000, early_stopping_rounds= 200) #(train_x, train_y), \n",
    "\n",
    "    oof_preds[valid_idx] = clf.predict_proba(valid_x)[:, 1]\n",
    "    sub_preds += clf.predict_proba(test_x)[:, 1] / folds.n_splits \n",
    "\n",
    "    del clf, train_x, train_y, valid_x, valid_y\n",
    "    gc.collect()\n",
    "\n",
    "print(roc_auc_score(y_train, oof_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "CatboostError",
     "evalue": "There is no trained model to use `feature_importances_`. Use fit() to train model. Then use `feature_importances_`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCatboostError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e4ce79c47429>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/anaconda3/lib/python3.6/site-packages/catboost/core.py\u001b[0m in \u001b[0;36mfeature_importances_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1409\u001b[0m         \u001b[0mfeature_importances_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_feature_importance\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fitted_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1411\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCatboostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"There is no trained model to use `feature_importances_`. Use fit() to train model. Then use `feature_importances_`.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1412\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCatboostError\u001b[0m: There is no trained model to use `feature_importances_`. Use fit() to train model. Then use `feature_importances_`."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START Stacking\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "NFOLDS = 5\n",
    "SEED = 47\n",
    "\n",
    "\n",
    "x_train = df[df['TARGET'].notnull()]\n",
    "x_test = df[df['TARGET'].isnull()]\n",
    "y_train = x_train['TARGET']\n",
    "ntrain = x_train.shape[0]\n",
    "ntest = x_test.shape[0]\n",
    "\n",
    "feats = [f for f in x_train.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "x_train = x_train[feats]\n",
    "x_test = x_test[feats]\n",
    "\n",
    "\n",
    "kf = KFold(n_splits = NFOLDS, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, df):\n",
    "    global kf, NFOLDS\n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    test_df = df[df['TARGET'].isnull()]\n",
    "    oof_train = np.zeros(train_df.shape[0])\n",
    "    oof_test = np.zeros(test_df.shape[0])\n",
    "    oof_test_skf = np.empty((NFOLDS, test_df.shape[0]))\n",
    "    \n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(kf.split(train_df[feats], train_df['TARGET'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "        \n",
    "        clf.train(train_x, train_y, valid_x, valid_y, test_df[feats])\n",
    "\n",
    "        oof_train[valid_idx] = clf.predict(valid_x)\n",
    "        oof_test_skf[n_fold, :] = clf.predict(test_df[feats])\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_params = {\n",
    "            'nthread': 8,\n",
    "            'n_estimators' : 10000,\n",
    "            'bagging_fraction' : 0.8028, \n",
    "            'bagging_freq' :  1,\n",
    "            'learning_rate' :  0.0076, \n",
    "            'num_leaves' : 39,\n",
    "            'colsample_bytree' :  0.8702,\n",
    "            'subsample' : 0.9033,\n",
    "            'max_depth' : 10,\n",
    "            'reg_alpha' : 0.1853,\n",
    "            'reg_lambda' : 0.0305,\n",
    "            'min_split_gain' : 0.0299,\n",
    "            'min_child_weight' : 45,\n",
    "            'silent' : -1,\n",
    "            'verbose' : -1,\n",
    "}\n",
    "\n",
    "\n",
    "catboost_params = {\n",
    "    'iterations': 5000, #200,\n",
    "    'learning_rate': 0.01,\n",
    "    'depth': 8, #3,\n",
    "    'l2_leaf_reg': 40,\n",
    "    'bootstrap_type': 'Bernoulli',\n",
    "    'subsample': 0.7,\n",
    "    'scale_pos_weight': 5,\n",
    "    'eval_metric': 'AUC',\n",
    "    'od_type': 'Iter',\n",
    "    'allow_writing_files': False,\n",
    "    'thread_count':8\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.7,\n",
    "    'learning_rate': 0.075,\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 4,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 1,\n",
    "    'nrounds': 5000,\n",
    "    'nthread':8\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': 500,\n",
    "    'max_depth':15,\n",
    "    'min_samples_split': 10,\n",
    "    'min_samples_leaf': 10,\n",
    "    'n_jobs': 8,\n",
    "    'verbose':0\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans(train, test1, test2):\n",
    "    feat = []\n",
    "    for col in train.columns:\n",
    "        s1 = train[col].isnull().sum()\n",
    "        s2 = test1[col].isnull().sum()\n",
    "        s3 = test2[col].isnull().sum()\n",
    "        \n",
    "        if (s1 == 0 and s2 != 0) or (s1 == 0 and s3 != 0):\n",
    "            pass\n",
    "        else:\n",
    "            feat.append(col)\n",
    "    return feat\n",
    "\n",
    "\n",
    "class LightGBMWrapper(object):\n",
    "    def __init__(self, clf, params=None):\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train, x_valid, y_valid, test_x=None):\n",
    "        self.clf.fit(x_train,  y_train, eval_set=[(x_train, y_train), (x_valid, y_valid)], \n",
    "            eval_metric= 'auc', verbose= 1000, early_stopping_rounds= 200)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict_proba(x, num_iteration=self.clf.best_iteration_)[:,1]\n",
    "    \n",
    "    \n",
    "class CatboostWrapper(object):\n",
    "    def __init__(self, clf, params=None):\n",
    "        self.clf = clf(**params)\n",
    "        self.feat = None\n",
    "\n",
    "    def train(self, train_x, train_y, valid_x, valid_y, test_x):\n",
    "        feat = trans(train_x, valid_x, test_x)\n",
    "        self.feat = feat\n",
    "        train_x = train_x[feat]\n",
    "        valid_x = valid_x[feat]\n",
    "        self.test_x = test_x[feat]\n",
    "        self.clf.fit(train_x, train_y, eval_set=[(valid_x, valid_y)],  verbose= 1000, early_stopping_rounds= 200)\n",
    "\n",
    "    def predict(self, x):\n",
    "        if x.shape[0] > 50000:\n",
    "            return self.clf.predict_proba(x[self.feat])[:,1]\n",
    "        else:\n",
    "            return self.clf.predict_proba(self.test_x)[:,1]\n",
    "            \n",
    "    \n",
    "    \n",
    "class XgbWrapper(object):\n",
    "    def __init__(self, clf, params=None):\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, train_x, train_y, valid_x, valid_y, test_x=None):\n",
    "        self.clf.fit(train_x, train_y, eval_set=[(valid_x, valid_y)],  verbose= 1000, eval_metric= 'auc', early_stopping_rounds= 200)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict_proba(x, ntree_limit=self.clf.best_iteration)[:,1]\n",
    "\n",
    "    \n",
    "class SklearnWrapper(object):\n",
    "    def __init__(self, clf, params=None):\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict_proba(x)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\ttraining's auc: 0.833492\tvalid_1's auc: 0.793486\n",
      "[2000]\ttraining's auc: 0.867891\tvalid_1's auc: 0.798864\n",
      "[3000]\ttraining's auc: 0.893009\tvalid_1's auc: 0.800139\n",
      "Early stopping, best iteration is:\n",
      "[3195]\ttraining's auc: 0.897251\tvalid_1's auc: 0.800307\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\ttraining's auc: 0.833249\tvalid_1's auc: 0.791476\n",
      "[2000]\ttraining's auc: 0.867395\tvalid_1's auc: 0.797022\n",
      "[3000]\ttraining's auc: 0.892247\tvalid_1's auc: 0.798361\n",
      "Early stopping, best iteration is:\n",
      "[3342]\ttraining's auc: 0.899578\tvalid_1's auc: 0.798663\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\ttraining's auc: 0.833262\tvalid_1's auc: 0.791537\n",
      "[2000]\ttraining's auc: 0.867962\tvalid_1's auc: 0.796499\n",
      "[3000]\ttraining's auc: 0.893032\tvalid_1's auc: 0.797499\n",
      "Early stopping, best iteration is:\n",
      "[3540]\ttraining's auc: 0.90426\tvalid_1's auc: 0.797746\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\ttraining's auc: 0.833424\tvalid_1's auc: 0.792621\n",
      "[2000]\ttraining's auc: 0.868226\tvalid_1's auc: 0.796983\n",
      "[3000]\ttraining's auc: 0.893281\tvalid_1's auc: 0.797746\n",
      "Early stopping, best iteration is:\n",
      "[2925]\ttraining's auc: 0.891547\tvalid_1's auc: 0.797802\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\ttraining's auc: 0.834626\tvalid_1's auc: 0.787258\n",
      "[2000]\ttraining's auc: 0.868586\tvalid_1's auc: 0.792348\n",
      "[3000]\ttraining's auc: 0.893262\tvalid_1's auc: 0.793674\n",
      "[4000]\ttraining's auc: 0.913279\tvalid_1's auc: 0.793938\n",
      "Early stopping, best iteration is:\n",
      "[3822]\ttraining's auc: 0.909913\tvalid_1's auc: 0.793957\n"
     ]
    }
   ],
   "source": [
    "#lg = LightGBMWrapper(clf = LGBMClassifier, params = lightgbm_params)\n",
    "#lg_oof_train, lg_oof_test = get_oof(lg, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.721069\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[99]\tvalidation_0-auc:0.780218\n",
      "[0]\tvalidation_0-auc:0.719487\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[99]\tvalidation_0-auc:0.777862\n",
      "[0]\tvalidation_0-auc:0.715321\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[99]\tvalidation_0-auc:0.778463\n",
      "[0]\tvalidation_0-auc:0.723927\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[99]\tvalidation_0-auc:0.780599\n",
      "[0]\tvalidation_0-auc:0.715127\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[99]\tvalidation_0-auc:0.775396\n"
     ]
    }
   ],
   "source": [
    "#xg = XgbWrapper(clf=xgb.XGBClassifier, params=xgb_params)\n",
    "#xg_oof_train, xg_oof_test = get_oof(xg, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.7281604\tbest: 0.7281604 (0)\ttotal: 609ms\tremaining: 50m 46s\n",
      "1000:\ttest: 0.7901143\tbest: 0.7901143 (1000)\ttotal: 10m 20s\tremaining: 41m 18s\n",
      "2000:\ttest: 0.7951261\tbest: 0.7951271 (1999)\ttotal: 20m 53s\tremaining: 31m 18s\n",
      "3000:\ttest: 0.7968563\tbest: 0.7968586 (2999)\ttotal: 31m 26s\tremaining: 20m 56s\n",
      "4000:\ttest: 0.7978842\tbest: 0.7978891 (3999)\ttotal: 41m 59s\tremaining: 10m 29s\n",
      "4999:\ttest: 0.7983966\tbest: 0.7984151 (4987)\ttotal: 52m 30s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7984151207\n",
      "bestIteration = 4987\n",
      "\n",
      "Shrink model to first 4988 iterations.\n",
      "0:\ttest: 0.7311363\tbest: 0.7311363 (0)\ttotal: 584ms\tremaining: 48m 41s\n",
      "1000:\ttest: 0.7890860\tbest: 0.7890860 (1000)\ttotal: 10m 13s\tremaining: 40m 51s\n",
      "2000:\ttest: 0.7938133\tbest: 0.7938133 (2000)\ttotal: 20m 38s\tremaining: 30m 55s\n",
      "3000:\ttest: 0.7956724\tbest: 0.7956806 (2974)\ttotal: 31m\tremaining: 20m 39s\n",
      "4000:\ttest: 0.7965442\tbest: 0.7965442 (4000)\ttotal: 41m 20s\tremaining: 10m 19s\n",
      "4999:\ttest: 0.7972134\tbest: 0.7972145 (4998)\ttotal: 51m 41s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7972144966\n",
      "bestIteration = 4998\n",
      "\n",
      "Shrink model to first 4999 iterations.\n",
      "0:\ttest: 0.7300974\tbest: 0.7300974 (0)\ttotal: 592ms\tremaining: 49m 21s\n",
      "1000:\ttest: 0.7890228\tbest: 0.7890228 (1000)\ttotal: 10m 14s\tremaining: 40m 56s\n",
      "2000:\ttest: 0.7935734\tbest: 0.7935778 (1993)\ttotal: 20m 40s\tremaining: 30m 59s\n",
      "3000:\ttest: 0.7951841\tbest: 0.7951841 (3000)\ttotal: 31m 2s\tremaining: 20m 40s\n",
      "4000:\ttest: 0.7960088\tbest: 0.7960146 (3984)\ttotal: 41m 26s\tremaining: 10m 20s\n",
      "Stopped by overfitting detector  (200 iterations wait)\n",
      "\n",
      "bestTest = 0.7962855201\n",
      "bestIteration = 4344\n",
      "\n",
      "Shrink model to first 4345 iterations.\n",
      "0:\ttest: 0.7333989\tbest: 0.7333989 (0)\ttotal: 573ms\tremaining: 47m 42s\n",
      "1000:\ttest: 0.7911961\tbest: 0.7911961 (1000)\ttotal: 10m 14s\tremaining: 40m 53s\n",
      "2000:\ttest: 0.7951364\tbest: 0.7951364 (2000)\ttotal: 20m 35s\tremaining: 30m 52s\n",
      "3000:\ttest: 0.7967668\tbest: 0.7967699 (2999)\ttotal: 30m 56s\tremaining: 20m 36s\n",
      "4000:\ttest: 0.7972040\tbest: 0.7972553 (3836)\ttotal: 41m 19s\tremaining: 10m 19s\n",
      "Stopped by overfitting detector  (200 iterations wait)\n",
      "\n",
      "bestTest = 0.7972553391\n",
      "bestIteration = 3836\n",
      "\n",
      "Shrink model to first 3837 iterations.\n",
      "0:\ttest: 0.7273987\tbest: 0.7273987 (0)\ttotal: 586ms\tremaining: 48m 48s\n",
      "1000:\ttest: 0.7843651\tbest: 0.7843651 (1000)\ttotal: 10m 12s\tremaining: 40m 48s\n",
      "2000:\ttest: 0.7883098\tbest: 0.7883098 (2000)\ttotal: 20m 34s\tremaining: 30m 50s\n",
      "3000:\ttest: 0.7899194\tbest: 0.7899293 (2997)\ttotal: 31m 2s\tremaining: 20m 40s\n",
      "4000:\ttest: 0.7907010\tbest: 0.7907116 (3992)\ttotal: 41m 28s\tremaining: 10m 21s\n"
     ]
    }
   ],
   "source": [
    "cb = CatboostWrapper(clf= CatBoostClassifier, params=catboost_params)\n",
    "cb_oof_train, cb_oof_test = get_oof(cb, df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = df.loc[:, ['SK_ID_CURR', 'TARGET']]\n",
    "\n",
    "res['LGB'] = np.append(lg_oof_train, lg_oof_test)\n",
    "res['XGB'] = np.append(xg_oof_train, xg_oof_test )\n",
    "res['CGB'] = np.append(cb_oof_train, cb_oof_test)\n",
    "\n",
    "res.to_csv(\"gb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ntrain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b92758d08e93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mlg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLightGBMWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSEED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlightgbm_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mxg_oof_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxg_oof_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_oof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;31m#et_oof_train, et_oof_test = get_oof(et)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m#rf_oof_train, rf_oof_test = get_oof(rf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-d923ab029742>\u001b[0m in \u001b[0;36mget_oof\u001b[0;34m(clf)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_oof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0moof_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mntrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0moof_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mntest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0moof_test_skf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNFOLDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ntrain' is not defined"
     ]
    }
   ],
   "source": [
    "et_params = {\n",
    "    'n_jobs': 16,\n",
    "    'n_estimators': 200,\n",
    "    'max_features': 0.5,\n",
    "    'max_depth': 12,\n",
    "    'min_samples_leaf': 2,\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'n_jobs': 16,\n",
    "    'n_estimators': 200,\n",
    "    'max_features': 0.2,\n",
    "    'max_depth': 12,\n",
    "    'min_samples_leaf': 2,\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.7,\n",
    "    'learning_rate': 0.075,\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 4,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 1,\n",
    "    'nrounds': 200\n",
    "}\n",
    "\n",
    "catboost_params = {\n",
    "    'iterations': 200,\n",
    "    'learning_rate': 0.5,\n",
    "    'depth': 3,\n",
    "    'l2_leaf_reg': 40,\n",
    "    'bootstrap_type': 'Bernoulli',\n",
    "    'subsample': 0.7,\n",
    "    'scale_pos_weight': 5,\n",
    "    'eval_metric': 'AUC',\n",
    "    'od_type': 'Iter',\n",
    "    'allow_writing_files': False\n",
    "}\n",
    "\n",
    "lightgbm_params = {\n",
    "    'n_estimators':200,\n",
    "    'learning_rate':0.1,\n",
    "    'num_leaves':123,\n",
    "    'colsample_bytree':0.8,\n",
    "    'subsample':0.9,\n",
    "    'max_depth':15,\n",
    "    'reg_alpha':0.1,\n",
    "    'reg_lambda':0.1,\n",
    "    'min_split_gain':0.01,\n",
    "    'min_child_weight':2    \n",
    "}\n",
    "\n",
    "xg = XgbWrapper(seed=SEED, params=xgb_params)\n",
    "#et = SklearnWrapper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
    "#rf = SklearnWrapper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n",
    "cb = CatboostWrapper(clf= CatBoostClassifier, seed = SEED, params=catboost_params)\n",
    "lg = LightGBMWrapper(clf = LGBMClassifier, seed = SEED, params = lightgbm_params)\n",
    "\n",
    "xg_oof_train, xg_oof_test = get_oof(xg)\n",
    "#et_oof_train, et_oof_test = get_oof(et)\n",
    "#rf_oof_train, rf_oof_test = get_oof(rf)\n",
    "cb_oof_train, cb_oof_test = get_oof(cb)\n",
    "\n",
    "print(\"XG-CV: {}\".format(sqrt(mean_squared_error(y_train, xg_oof_train))))\n",
    "print(\"ET-CV: {}\".format(sqrt(mean_squared_error(y_train, et_oof_train))))\n",
    "print(\"RF-CV: {}\".format(sqrt(mean_squared_error(y_train, rf_oof_train))))\n",
    "print(\"RF-CV: {}\".format(sqrt(mean_squared_error(y_train, cb_oof_train))))\n",
    "\n",
    "x_train = np.concatenate((xg_oof_train, et_oof_train, rf_oof_train, cb_oof_train), axis=1)\n",
    "x_test = np.concatenate((xg_oof_test, et_oof_test, rf_oof_test, cb_oof_test), axis=1)\n",
    "\n",
    "print(\"{},{}\".format(x_train.shape, x_test.shape))\n",
    "\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(x_train,y_train)\n",
    "\n",
    "test['TARGET'] = logistic_regression.predict_proba(x_test)[:,1]\n",
    "\n",
    "test[['SK_ID_CURR', 'TARGET']].to_csv('first_submission.csv', index=False, float_format='%.8f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
