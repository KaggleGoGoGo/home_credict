{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandle:\n",
    "    def __init__(self, missing_rate=0.3):\n",
    "        self.missing_rate = missing_rate\n",
    "        self.clear()\n",
    "    \n",
    "    def clear(self):\n",
    "        self.imputer = None\n",
    "        self.ss = None\n",
    "        self.sfm = None\n",
    "        self.x_before = None\n",
    "        self.x_after = None\n",
    "        self.feature_by_missing = None\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        self.clear()\n",
    "        if not isinstance(x, pd.DataFrame):\n",
    "            raise TypeError\n",
    "        self.x_before = x\n",
    "        self.y_before = y\n",
    "        self.y_after = y\n",
    "        self.feature_by_missing = x.columns[x.isnull().sum()/x.shape[0] < self.missing_rate]\n",
    "        self.x_before = self.x_before[self.feature_by_missing]\n",
    "        self.x_before = pd.get_dummies(self.x_before)\n",
    "        self.x_after = self.data_preprocess(self.x_before.values, y)\n",
    "        \n",
    "    def data_preprocess(self, x, y=None):\n",
    "        assert isinstance(x, np.ndarray)\n",
    "        # 异常值\n",
    "        pass\n",
    "\n",
    "        # 缺失值\n",
    "        if self.imputer is None:\n",
    "            self.imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "            self.imputer.fit(x)\n",
    "        x =  self.imputer.transform(x)\n",
    "\n",
    "        # 标准化\n",
    "        if self.ss is None:\n",
    "            self.ss = StandardScaler()\n",
    "            self.ss.fit(x)\n",
    "        x = self.ss.transform(x)\n",
    "\n",
    "        # 特征选择\n",
    "        if self.sfm is None:\n",
    "            self.sfm = SelectFromModel(LogisticRegression(penalty=\"l1\", C=0.01))\n",
    "            self.sfm.fit(x, y)\n",
    "        x = self.sfm.transform(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def transform(self, x_test):\n",
    "        x_test = x_test[self.feature_by_missing]\n",
    "        x_test = pd.get_dummies(x_test)\n",
    "        # 统一train和test的特征\n",
    "        _, x_test = self.x_before.align(x_test, join='left', axis=1, fill_value=0)\n",
    "        x_test = self.data_preprocess(x_test.values)\n",
    "        return self.x_after, self.y_after, x_test\n",
    "\n",
    "\n",
    "def get_auc_score(y_true, y_predict_proba):\n",
    "    f, t, _ = roc_curve(y_true, y_predict_proba, pos_label=1)\n",
    "    return auc(f, t)\n",
    "\n",
    "def output(test_id, test_prob, sid=0):\n",
    "    result = pd.DataFrame(np.column_stack((test_id, test_prob)))\n",
    "    result.columns = ['SK_ID_CURR', 'TARGET']\n",
    "    result['SK_ID_CURR'] = result['SK_ID_CURR'].astype('int')\n",
    "    result.to_csv('submission' + str(sid) + '.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/application_train.csv\")\n",
    "df_test = pd.read_csv(\"../data/application_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全部数据集，用以输出并且submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_all = df_train.iloc[:, 2:]\n",
    "y_train_all = df_train['TARGET'].values\n",
    "\n",
    "x_test_all = df_test.iloc[:, 1:]\n",
    "test_id  = df_test.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_all = DataHandle()\n",
    "dh_all.fit(x_train_all, y_train_all)\n",
    "x_train_all, y_train_all, x_test_all = dh_all.transform(x_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511, 118)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 切分数据集，用以获得模型参数并且交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = df_train.iloc[:, 2:]\n",
    "df_y = df_train['TARGET']\n",
    "df_x_train, df_x_test, df_y_train, df_y_test = train_test_split(df_x, df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = DataHandle()\n",
    "dh.fit(df_x_train, df_y_train)\n",
    "x_train, y_train, x_test = dh.transform(df_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m----------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   learning_rate |   max_depth |   min_samples_leaf |   min_samples_split |   n_estimators |   sub_sample | \n",
      "    1 | 00m12s | \u001b[35m   0.68233\u001b[0m | \u001b[32m         0.0010\u001b[0m | \u001b[32m     2.0000\u001b[0m | \u001b[32m           10.0000\u001b[0m | \u001b[32m            10.0000\u001b[0m | \u001b[32m       20.0000\u001b[0m | \u001b[32m      0.1000\u001b[0m | \n",
      "    2 | 02m49s | \u001b[35m   0.74365\u001b[0m | \u001b[32m         0.1000\u001b[0m | \u001b[32m     3.0000\u001b[0m | \u001b[32m           40.0000\u001b[0m | \u001b[32m            50.0000\u001b[0m | \u001b[32m      100.0000\u001b[0m | \u001b[32m      0.5000\u001b[0m | \n",
      "    3 | 14m00s |    0.72152 |          0.5000 |      4.0000 |            80.0000 |             80.0000 |       200.0000 |       0.8000 | \n",
      "    4 | 26m02s |    0.68976 |          0.5825 |      5.8895 |            60.9732 |             34.6326 |       251.1589 |       0.8872 | \n",
      "    5 | 04m50s |    0.71701 |          0.3805 |      2.9933 |            20.6065 |             60.2386 |       322.1731 |       0.0989 | \n",
      "    6 | 05m26s |    0.68972 |          0.4713 |      2.3718 |            16.9349 |             69.6162 |       412.9304 |       0.0732 | \n",
      "    7 | 04m05s |    0.73858 |          0.0449 |      2.6084 |            91.7738 |             67.9170 |       444.2404 |       0.0283 | \n",
      "    8 | 06m00s |    0.73543 |          0.6924 |      2.4826 |            46.4887 |             75.6997 |       158.7864 |       0.5914 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m----------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   learning_rate |   max_depth |   min_samples_leaf |   min_samples_split |   n_estimators |   sub_sample | \n",
      "    9 | 00m43s |    0.70313 |          0.0010 |      8.0000 |           100.0000 |            100.0000 |        10.0000 |       0.0100 | \n",
      "   10 | 08m09s |    0.73300 |          0.2375 |      7.8732 |            10.2256 |             98.8423 |        36.8437 |       0.6252 | \n",
      "   11 | 10m38s |    0.73449 |          0.3486 |      2.3734 |            97.9281 |             98.9663 |       496.3727 |       0.2098 | \n",
      "   12 | 30m23s |    0.63900 |          0.9533 |      5.4232 |            96.7580 |             10.4220 |       496.1090 |       0.9701 | \n",
      "   13 | 16m08s | \u001b[35m   0.74591\u001b[0m | \u001b[32m         0.1335\u001b[0m | \u001b[32m     4.3234\u001b[0m | \u001b[32m           99.6282\u001b[0m | \u001b[32m            99.5506\u001b[0m | \u001b[32m      368.1656\u001b[0m | \u001b[32m      0.9438\u001b[0m | \n",
      "   14 | 10m53s |    0.73192 |          0.2075 |      7.9509 |            12.1439 |             98.7455 |       122.7647 |       0.8196 | \n",
      "   15 | 02m05s |    0.74381 |          0.3476 |      2.0049 |            99.7121 |             98.3720 |       107.7199 |       0.5367 | \n",
      "   16 | 00m37s |    0.73605 |          0.4238 |      2.2360 |            99.6358 |             20.0932 |        63.0138 |       0.1668 | \n",
      "   17 | 01m32s |    0.74261 |          0.4963 |      2.1639 |            42.2395 |             96.6081 |        79.7937 |       0.5946 | \n",
      "   18 | 60m08s |    0.61990 |          0.9740 |      7.7891 |            99.8407 |             98.9094 |       425.5980 |       0.7393 | \n"
     ]
    }
   ],
   "source": [
    "# GBDT调参\n",
    "\n",
    "y_train = np.array(y_train, dtype='int')\n",
    "y_test = np.array(df_y_test, dtype='int')\n",
    "\n",
    "gp_params = {\"alpha\": 1e-5}\n",
    "\n",
    "\n",
    "def gbt_cv(learning_rate, n_estimators, max_depth, min_samples_split, min_samples_leaf,sub_sample):\n",
    "    val = cross_val_score(\n",
    "        GradientBoostingClassifier(learning_rate=learning_rate, n_estimators=int(n_estimators),\n",
    "                                  max_depth=int(max_depth), min_samples_split=int(min_samples_split),\n",
    "                                  min_samples_leaf=int(min_samples_leaf), subsample=sub_sample),\n",
    "        x_train, y_train, 'roc_auc', cv=2\n",
    "    ).mean()\n",
    "\n",
    "    return val\n",
    "\n",
    "svcBO = BayesianOptimization(gbt_cv,\n",
    "                             {'learning_rate': (0.001, 1), 'n_estimators': (10, 500),\n",
    "                             'max_depth': (2, 8), 'min_samples_split': (10, 100), \n",
    "                             'min_samples_leaf':(10, 100), 'sub_sample': (0.01, 1.0)})\n",
    "\n",
    "svcBO.explore({'learning_rate': [0.001, 0.1, 0.5], 'n_estimators': [20, 100, 200],\n",
    "                             'max_depth': [2,3,4], 'min_samples_split': [10, 50, 80], \n",
    "                             'min_samples_leaf': [10, 40, 80], 'sub_sample':[0.1,0.5,0.8]})\n",
    "svcBO.maximize(n_iter=10, **gp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_val': 0.7459098616411393, 'max_params': {'learning_rate': 0.13349744579104986, 'n_estimators': 368.16555294973546, 'max_depth': 4.323369081020995, 'min_samples_split': 99.55056500297515, 'min_samples_leaf': 99.62824252348736, 'sub_sample': 0.9438267090259804}}\n"
     ]
    }
   ],
   "source": [
    "print(svcBO.res['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7521702069313638\n"
     ]
    }
   ],
   "source": [
    "gbdt = GradientBoostingClassifier(learning_rate=0.1335, n_estimators=368, max_depth=4,\n",
    "                                  min_samples_leaf=100, min_samples_split=100, subsample=0.9438)\n",
    "gbdt.fit(x_train, y_train)\n",
    "y_pred = gbdt.predict_proba(x_test)[:, 1]\n",
    "print(get_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当前最好的GBDT参数\n",
    "gbdt = GradientBoostingClassifier(learning_rate=0.1335, n_estimators=368, max_depth=4,\n",
    "                                  min_samples_leaf=100, min_samples_split=100, subsample=0.9438)\n",
    "gbdt.fit(x_train_all, y_train_all)\n",
    "y_pred = gbdt.predict_proba(x_test_all)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output(test_id, y_pred, sid=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林调参\n",
    "\n",
    "gp_params = {\"alpha\": 1e-5}\n",
    "\n",
    "def rf_cv(n_estimators, max_depth, max_features, min_samples_leaf, min_samples_split):\n",
    "    val = cross_val_score(\n",
    "        RandomForestClassifier(criterion=\"gini\", n_estimators=int(n_estimators),\n",
    "                               max_depth=int(max_depth),\n",
    "                               max_features=int(max_features),\n",
    "                               min_samples_leaf=int(min_samples_leaf), \n",
    "                               min_samples_split=int(min_samples_split)),\n",
    "        x_train, y_train, 'roc_auc', cv=2\n",
    "    ).mean()\n",
    "    return val\n",
    "\n",
    "svcBO = BayesianOptimization(rf_cv,\n",
    "                             {'n_estimators':(10,500), 'max_depth': (2, 20), 'max_features': (1, x_train.shape[1]-1),\n",
    "                             'min_samples_leaf': (2, 100), 'min_samples_split': (2, 100)})\n",
    "\n",
    "svcBO.explore({'n_estimators': [10, 200,400], 'max_depth': [2, 10, 20], 'max_features': [1, x_train.shape[1]//2, x_train.shape[1]-1],\n",
    "                             'min_samples_leaf': [2,50,100], 'min_samples_split': [2, 50, 100]})\n",
    "svcBO.maximize(n_iter=10, **gp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "clf = lgb.LGBMClassifier( \n",
    "            nthread=4,\n",
    "            n_estimators=10000,\n",
    "            learning_rate=0.02,\n",
    "            num_leaves=34,\n",
    "            colsample_bytree=0.9497036,\n",
    "            subsample=0.8715623,\n",
    "            max_depth=8,\n",
    "            reg_alpha=0.041545473,\n",
    "            reg_lambda=0.0735294,\n",
    "            min_split_gain=0.0222415,\n",
    "            min_child_weight=39.3259775,\n",
    "            silent=-1,\n",
    "            verbose=-1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(x_train, y_train)\n",
    "pred = clf.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49833144085681846"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_auc_score(y_test, pred[:, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
