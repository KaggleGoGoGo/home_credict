{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这个文件做了什么\n",
    "\n",
    "- XGBoost 11模型处理数据不平衡（为啥是11，因为正负样本比是1:11）\n",
    "- LightGBM 11模型，获得最优结果0.772"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LassoCV\n",
    "import gc\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "def data_prepare(num=None):\n",
    "    train = pd.read_csv('../data/handled/train.csv', nrows=num, index_col=0)\n",
    "    test = pd.read_csv('../data/handled/test.csv', nrows=num, index_col=0)\n",
    "    y_train = pd.read_csv('../data/handled/y_train.csv', nrows=num, header=-1, index_col=0)\n",
    "    return train, test, y_train\n",
    "\n",
    "\n",
    "def output_result(test_id, test_prob, sid=''):\n",
    "    result = pd.DataFrame(np.column_stack((test_id, test_prob)))\n",
    "    result.columns = ['SK_ID_CURR', 'TARGET']\n",
    "    result['SK_ID_CURR'] = result['SK_ID_CURR'].astype('int')\n",
    "    result.to_csv('./submission/submission_' + str(sid) + '.csv', header=True, index=False)\n",
    "\n",
    "\n",
    "def show_importance(model, num=20, height=0.8):\n",
    "    xgb.plot_importance(model, max_num_features=num, height=height)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def xgb_k_folder_cv(params, xgtrain, fold=5, seed=918):\n",
    "    cv = xgb.cv(params, xgtrain, metrics='auc', early_stopping_rounds=50,\n",
    "                nfold=fold, seed=seed)\n",
    "    return cv   # ['test-auc-mean'].values[-1]\n",
    "\n",
    "\n",
    "def xgb_evaluate(params,\n",
    "                 xgtrain,\n",
    "                 #   以下需要再次调用匿名函数封装\n",
    "                 eta,\n",
    "                 min_child_weight,\n",
    "                 cosample_bytree,\n",
    "                 max_depth,\n",
    "                 subsample,\n",
    "                 gamma,\n",
    "                 alpha):\n",
    "    params['eta'] = max(eta, 0)\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['cosample_bytree'] = max(min(cosample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    params['alpha'] = max(alpha, 0)\n",
    "\n",
    "    cv = xgb.cv(params, xgtrain, metrics='auc', early_stopping_rounds=50,\n",
    "                nfold=5, seed=918)\n",
    "    return cv['test-auc-mean'].values[-1]\n",
    "\n",
    "\n",
    "def xgb_no_feature_select(train: pd.DataFrame, test: pd.DataFrame, y_train, cv=False):\n",
    "    params = {\n",
    "        'silent': 1,\n",
    "        'nthread': 4,\n",
    "        'eval_metric': 'auc',\n",
    "        'verbose_eval': True,\n",
    "        'seed': 918,\n",
    "        'alpha': 9.6523,\n",
    "        'cosample_bytree': 0.9604,\n",
    "        'eta': 0.1171,\n",
    "        'gamma': 0.179,\n",
    "        'max_depth': 7,\n",
    "        'min_child_weight': 13,\n",
    "        'subsample': 0.9609\n",
    "    }\n",
    "    xgtrain = xgb.DMatrix(train, label=y_train)\n",
    "    if cv:\n",
    "        cv_res = xgb_k_folder_cv(params, xgtrain)\n",
    "        print(cv_res)\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(train, y_train)\n",
    "    y_predict = model.predict_proba(test)\n",
    "    return model, y_predict\n",
    "\n",
    "\n",
    "def xgb_feature_select(train, test, y_train, importance, top_num=None, cv=False):\n",
    "    if top_num:\n",
    "        threshold = np.sort(importance)[-top_num-1]\n",
    "    else:\n",
    "        threshold = 0\n",
    "    select_id = [True if i > threshold else False for i in importance]\n",
    "    train = train.loc[:, select_id]\n",
    "    test = test.loc[:, select_id]\n",
    "    params = {\n",
    "        'silent': 1,\n",
    "        'nthread': 4,\n",
    "        'eval_metric': 'auc',\n",
    "        'verbose_eval': True,\n",
    "        'seed': 918,\n",
    "        'alpha': 9.6523,\n",
    "        'cosample_bytree': 0.9604,\n",
    "        'eta': 0.1171,\n",
    "        'gamma': 0.179,\n",
    "        'max_depth': 7,\n",
    "        'min_child_weight': 13,\n",
    "        'subsample': 0.9609\n",
    "    }\n",
    "    xgtrain = xgb.DMatrix(train, label=y_train)\n",
    "    xgtest = xgb.DMatrix(test)\n",
    "    if cv:\n",
    "        cv_res = xgb_k_folder_cv(params, xgtrain)\n",
    "        print(cv_res)\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(train, y_train)\n",
    "    y_predict = model.predict_proba(test)\n",
    "    return model, y_predict\n",
    "\n",
    "\n",
    "def xgb_bayes_opt(train, y_train):\n",
    "    # num_rounds = 3000\n",
    "    random_state = 918\n",
    "    num_iter = 25\n",
    "    init_points = 5\n",
    "    params = {\n",
    "        'silent': 1,\n",
    "        'nthread': 4,\n",
    "        'eval_metric': 'auc',\n",
    "        'verbose_eval': True,\n",
    "        'seed': random_state,\n",
    "    }\n",
    "    xgtrian = xgb.DMatrix(train, label=y_train)\n",
    "    _xgb_evaluate = lambda a, b, c, d, e, f, g: xgb_evaluate(params, xgtrian, a, b, c, d, e, f, g)\n",
    "    xgbBO = BayesianOptimization(_xgb_evaluate, {\n",
    "        'eta': (0.1, 0.5),\n",
    "        'min_child_weight': (1, 20),\n",
    "        'cosample_bytree': (0.1, 1),\n",
    "        'max_depth': (5, 15),\n",
    "        'subsample': (0.5, 1),\n",
    "        'gamma': (0, 10),\n",
    "        'alpha': (0, 10)\n",
    "    })\n",
    "    xgbBO.maximize(init_points=init_points, n_iter=num_iter)\n",
    "    return xgbBO\n",
    "\n",
    "\n",
    "def xgb_unbalance_handle(train, test):\n",
    "    pass\n",
    "\n",
    "\n",
    "def models_stack(trian, test):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, y_train  = data_prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Imputer()\n",
    "train = im.fit_transform(df_train.values)\n",
    "test = im.transform(df_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307511"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = np.arange(df_train.shape[0])\n",
    "np.random.seed(918)\n",
    "np.random.shuffle(idxs)\n",
    "\n",
    "idx_list = []\n",
    "num = df_train.shape[0] // 11\n",
    "for i in range(11):\n",
    "    if i != 0:\n",
    "        idx_list.append(idxs[i * num:(i+1) * num])\n",
    "    else:\n",
    "        idx_list.append(idxs[10 * num: ])\n",
    "\n",
    "length = 0\n",
    "for i in idx_list:\n",
    "    length += len(i)\n",
    "length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'silent': 1,\n",
    "        'nthread': 4,\n",
    "        'eval_metric': 'auc',\n",
    "        'verbose_eval': True,\n",
    "        'seed': 918,\n",
    "        'alpha': 9.6523,\n",
    "        'cosample_bytree': 0.9604,\n",
    "        'eta': 0.1171,\n",
    "        'gamma': 0.179,\n",
    "        'max_depth': 7,\n",
    "        'min_child_weight': 13,\n",
    "        'subsample': 0.9609\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_list = [XGBClassifier(**params) for i in range(11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "for i, idx in enumerate(idx_list):\n",
    "    new_train = train[idx, :]\n",
    "    new_y_train = y_train.values[idx]\n",
    "    cls_list[i].fit(new_train, new_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_list = []\n",
    "for cls in cls_list:\n",
    "    predict_list.append(cls.predict_proba(test)[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.mean(np.array(predict_list), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_result(df_test.index, res, sid='_11models_mean')  # 0.769"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 0.772的参数\n",
    "# params_lgb = {\n",
    "#     'nthread': 4,\n",
    "#     #is_unbalance=True,\n",
    "#     'n_estimators' : 10000,\n",
    "#     'learning_rate' : 0.1171,\n",
    "#     #'num_leaves' : 32,\n",
    "#     'colsample_bytree' : 0.9604,\n",
    "#     'subsample' : 0.9609,\n",
    "#     'max_depth' : 7,\n",
    "#     'reg_alpha' : 9.6523,\n",
    "#     'reg_lambda' : 1,\n",
    "#     'min_split_gain' : 0.179,\n",
    "#     'min_child_weight' : 13,\n",
    "#     'metric': 'auc',\n",
    "#     'silent': -1,\n",
    "#     'verbose': -1,\n",
    "#     #scale_pos_weight=11\n",
    "# }\n",
    "\n",
    "# 测试参数\n",
    "params_lgb = {\n",
    "    'nthread': 4,\n",
    "    'learning_rate' : 0.1,\n",
    "    'n_estimators' : 10000,\n",
    "    'max_depth' : 14,\n",
    "    'min_child_weight' : 9,\n",
    "    'num_leaves': 50,\n",
    "    'colsample_bytree' : 0.9604,\n",
    "    'subsample' : 0.9609,\n",
    "    'reg_alpha' : 1,\n",
    "    'reg_lambda' : 1,\n",
    "    'min_split_gain': 0.179,\n",
    "    'metric': 'auc', \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = lgb.LGBMClassifier(**params_lgb)\n",
    "# clf.fit(train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./importance.txt', 'w') as f:\n",
    "#     f.write(\",\".join([str(i) for i in clf.feature_importances_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({True: 494, False: 235})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./importance.txt', 'r') as f:\n",
    "    imp = f.readline()\n",
    "imp = np.array([float(i) for i in imp.split(\",\")])\n",
    "Counter(imp!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:394: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'auc-mean': [0.7126279264511636,\n",
       "  0.7197547605101197,\n",
       "  0.7247751276220311,\n",
       "  0.7279719631060804,\n",
       "  0.7306819781017511,\n",
       "  0.7328141498283063,\n",
       "  0.7349241747183818,\n",
       "  0.7373736612831365,\n",
       "  0.7391132953490921,\n",
       "  0.7410615901503937,\n",
       "  0.7425229933110855,\n",
       "  0.7440041700798464,\n",
       "  0.7455445239450417,\n",
       "  0.7467603001257452,\n",
       "  0.7481734541327923,\n",
       "  0.7492153812907109,\n",
       "  0.7502546190981512,\n",
       "  0.7515502058590728,\n",
       "  0.7528323601262417,\n",
       "  0.7537306188279674,\n",
       "  0.7549260717300712,\n",
       "  0.7560719958336332,\n",
       "  0.7568125000569209,\n",
       "  0.7578306432866746,\n",
       "  0.7587043063756489,\n",
       "  0.7595713020567706,\n",
       "  0.7604552977590439,\n",
       "  0.7610956963967888,\n",
       "  0.7617481502384162,\n",
       "  0.7622708471671141,\n",
       "  0.7628948955516244,\n",
       "  0.7635365755882105,\n",
       "  0.7639369505255551,\n",
       "  0.7644087974557694,\n",
       "  0.7647509586025716,\n",
       "  0.7650497692493698,\n",
       "  0.7655410058266116,\n",
       "  0.7657865157869506,\n",
       "  0.76614172639534,\n",
       "  0.7665247165544494,\n",
       "  0.7668657935940788,\n",
       "  0.7671633095376243,\n",
       "  0.7674062875940078,\n",
       "  0.7676167898025055,\n",
       "  0.7678919445824154,\n",
       "  0.7682043572400138,\n",
       "  0.7685043208840444,\n",
       "  0.7687635387254188,\n",
       "  0.7688881041768456,\n",
       "  0.7691114976441663,\n",
       "  0.7693401866824388,\n",
       "  0.7695435124484247,\n",
       "  0.7697596421206098,\n",
       "  0.7699757067012857,\n",
       "  0.7702032347144057,\n",
       "  0.7703615635493455,\n",
       "  0.7704884066463739,\n",
       "  0.7706475601080706,\n",
       "  0.7708250245514264,\n",
       "  0.7709826926651553,\n",
       "  0.7711637429418692,\n",
       "  0.771346834453198,\n",
       "  0.7714842824702318,\n",
       "  0.7715676202540633,\n",
       "  0.7717961349633834,\n",
       "  0.7718517771229595,\n",
       "  0.7719978335010821,\n",
       "  0.7721333900077127,\n",
       "  0.7722427620583704,\n",
       "  0.7723161258267414,\n",
       "  0.7724332548047539,\n",
       "  0.7725218379119534,\n",
       "  0.7725762132333387,\n",
       "  0.7726508887728913,\n",
       "  0.7727495660040077,\n",
       "  0.7728494105408998,\n",
       "  0.7729273327798936,\n",
       "  0.773041473822426,\n",
       "  0.7731282929494236,\n",
       "  0.7731968849456359,\n",
       "  0.7733423439630192,\n",
       "  0.773415791199188,\n",
       "  0.7734428908534193,\n",
       "  0.7735137142740914,\n",
       "  0.7735440091707586,\n",
       "  0.7736095078104543,\n",
       "  0.7736911394740112,\n",
       "  0.7738026535065912,\n",
       "  0.7738830083228562,\n",
       "  0.7739385360647542,\n",
       "  0.7740312580284824,\n",
       "  0.7740600829180511,\n",
       "  0.7741110369837547,\n",
       "  0.7741715788374229,\n",
       "  0.7742167279779546,\n",
       "  0.7742426820163713,\n",
       "  0.7742875299023504,\n",
       "  0.774385326849098,\n",
       "  0.774401645116415,\n",
       "  0.7744277482376806,\n",
       "  0.7745320480747289,\n",
       "  0.7745965373997403,\n",
       "  0.7746872849875366,\n",
       "  0.7747232928431703,\n",
       "  0.7747779120429923,\n",
       "  0.7748359679892413,\n",
       "  0.7748510145185117,\n",
       "  0.774855393564703,\n",
       "  0.7749126513555978,\n",
       "  0.7749560588004021,\n",
       "  0.7749878349046153,\n",
       "  0.7749836935672525,\n",
       "  0.7750540631611942,\n",
       "  0.7751158418511588,\n",
       "  0.7751356054679132,\n",
       "  0.7751505735332649,\n",
       "  0.7752100416593469,\n",
       "  0.7752674808669391,\n",
       "  0.775278140602208,\n",
       "  0.7753049635657889,\n",
       "  0.7753274436138415,\n",
       "  0.775345693404577,\n",
       "  0.7754083018905251,\n",
       "  0.7754179921001523,\n",
       "  0.7754577738732793,\n",
       "  0.7755133925705857,\n",
       "  0.7755431768348459,\n",
       "  0.7756017819890652,\n",
       "  0.7755848158318048,\n",
       "  0.775621792893502,\n",
       "  0.7756333225327616,\n",
       "  0.7757093709727667,\n",
       "  0.7757697327915662,\n",
       "  0.7757933265739609,\n",
       "  0.77583005567624,\n",
       "  0.775814604082165,\n",
       "  0.7757932366706874,\n",
       "  0.7757533460072473,\n",
       "  0.7757289103886758,\n",
       "  0.7757173917735625,\n",
       "  0.7757475030976638,\n",
       "  0.7757819125061604,\n",
       "  0.7757860433176329,\n",
       "  0.7758204247025017,\n",
       "  0.7758741235515403,\n",
       "  0.7758633219002933,\n",
       "  0.7758667268833624,\n",
       "  0.7758609695218098,\n",
       "  0.7758933964001633,\n",
       "  0.7758888501962387,\n",
       "  0.7758961186431403,\n",
       "  0.7759043763245641,\n",
       "  0.7759078513716007,\n",
       "  0.775916007813293,\n",
       "  0.7759240334071954,\n",
       "  0.775922655616782,\n",
       "  0.7759220153743586,\n",
       "  0.7759222953041406,\n",
       "  0.7759257820869422],\n",
       " 'auc-stdv': [0.0025090603560906045,\n",
       "  0.0020699153727040138,\n",
       "  0.0025816966752601283,\n",
       "  0.0020742362850858743,\n",
       "  0.0015963300343578657,\n",
       "  0.0011788781066635662,\n",
       "  0.0011695000561528974,\n",
       "  0.0010392756859878944,\n",
       "  0.0011607449840120958,\n",
       "  0.0015584982446214374,\n",
       "  0.0018581271978978145,\n",
       "  0.0018266045335234636,\n",
       "  0.0018310348496354378,\n",
       "  0.002121449605621799,\n",
       "  0.0019624907118042053,\n",
       "  0.0021333884146186232,\n",
       "  0.002027669160646323,\n",
       "  0.001935274144227852,\n",
       "  0.00214056199734901,\n",
       "  0.0021656764513862566,\n",
       "  0.002204977476751331,\n",
       "  0.002090503934469511,\n",
       "  0.002089094151478872,\n",
       "  0.002011720679959723,\n",
       "  0.0022052940141913373,\n",
       "  0.002245143737066127,\n",
       "  0.0023093594215461023,\n",
       "  0.0021878073572319415,\n",
       "  0.0023298184459559987,\n",
       "  0.0023835149401251043,\n",
       "  0.0023835604608946188,\n",
       "  0.002291631826378647,\n",
       "  0.0022217794795295225,\n",
       "  0.002142506200646182,\n",
       "  0.002170129829656237,\n",
       "  0.002276371814838145,\n",
       "  0.002302670674272084,\n",
       "  0.002231385061681098,\n",
       "  0.0022488406405626058,\n",
       "  0.0021863686483143173,\n",
       "  0.0022256473639800513,\n",
       "  0.002192057729055073,\n",
       "  0.0021582471069148744,\n",
       "  0.0021941009103766547,\n",
       "  0.0022079379568171694,\n",
       "  0.002235858368864283,\n",
       "  0.0020434281220247444,\n",
       "  0.0021142968415994706,\n",
       "  0.00209066433452682,\n",
       "  0.0020552878483773933,\n",
       "  0.0020567757059846976,\n",
       "  0.0020683021438189205,\n",
       "  0.002067950154716621,\n",
       "  0.0020530323046939313,\n",
       "  0.0020602832265954212,\n",
       "  0.0020617233421780685,\n",
       "  0.002120240479197298,\n",
       "  0.002134809104112128,\n",
       "  0.002188284534519887,\n",
       "  0.002140911797901236,\n",
       "  0.002202601196637197,\n",
       "  0.0022520436011218815,\n",
       "  0.0022434101323406544,\n",
       "  0.0022639539548808593,\n",
       "  0.0024012703734649084,\n",
       "  0.0023793950818963793,\n",
       "  0.002376263842104191,\n",
       "  0.002293344512556497,\n",
       "  0.0023226503404492724,\n",
       "  0.0023710310627720124,\n",
       "  0.0023582718378569946,\n",
       "  0.0023304867430813834,\n",
       "  0.002324688466378171,\n",
       "  0.002373497479900603,\n",
       "  0.0023499369517149286,\n",
       "  0.0023337083851987933,\n",
       "  0.002338427857979308,\n",
       "  0.002321672346780827,\n",
       "  0.00228138356068497,\n",
       "  0.002293264771348509,\n",
       "  0.002337684695570698,\n",
       "  0.0023957197507683227,\n",
       "  0.0023323836449595592,\n",
       "  0.0023236669308968365,\n",
       "  0.002331316660412603,\n",
       "  0.002358218778818652,\n",
       "  0.002328168500747949,\n",
       "  0.0023107103375498536,\n",
       "  0.0022748859873734724,\n",
       "  0.002250018626112459,\n",
       "  0.0023239902558158214,\n",
       "  0.0022313293322509553,\n",
       "  0.002209305020804688,\n",
       "  0.0021807535131687194,\n",
       "  0.002142695015604325,\n",
       "  0.002140605915487616,\n",
       "  0.0021403419874491472,\n",
       "  0.0020995236594330115,\n",
       "  0.002068848122997699,\n",
       "  0.0020628906012177214,\n",
       "  0.002083512633250636,\n",
       "  0.002097215582014344,\n",
       "  0.002123964169944279,\n",
       "  0.0021587495216679503,\n",
       "  0.002153860777058798,\n",
       "  0.0021626120187846188,\n",
       "  0.002169115947484325,\n",
       "  0.0021899186961928433,\n",
       "  0.0021507413517056163,\n",
       "  0.0021867565288169744,\n",
       "  0.0021853588547800598,\n",
       "  0.0022675525464212324,\n",
       "  0.002239067293517009,\n",
       "  0.002209994139184162,\n",
       "  0.0021708984797794754,\n",
       "  0.0021908677458598214,\n",
       "  0.0022007018492576926,\n",
       "  0.0022130854809299587,\n",
       "  0.002204510507828226,\n",
       "  0.0022244008799092254,\n",
       "  0.0021951814631047577,\n",
       "  0.002189893380050262,\n",
       "  0.002240875821885167,\n",
       "  0.0022113384345427314,\n",
       "  0.00222381485776005,\n",
       "  0.0021985078257873714,\n",
       "  0.002181161870853314,\n",
       "  0.00222088758590746,\n",
       "  0.0022169657484793614,\n",
       "  0.002260740199421686,\n",
       "  0.00223322563517897,\n",
       "  0.002238806438056945,\n",
       "  0.0022435635219842866,\n",
       "  0.0022376493213433,\n",
       "  0.002272535796573746,\n",
       "  0.002272673033675477,\n",
       "  0.002304593040455638,\n",
       "  0.0022955139190517877,\n",
       "  0.002319777970363012,\n",
       "  0.002336605716545675,\n",
       "  0.0023181720728956733,\n",
       "  0.002356606515516709,\n",
       "  0.0023620780737324885,\n",
       "  0.0024017426887027377,\n",
       "  0.0024251211688258232,\n",
       "  0.0024387950811363083,\n",
       "  0.0024419703073726533,\n",
       "  0.0024493889070644694,\n",
       "  0.002447706977215471,\n",
       "  0.0024471946933558517,\n",
       "  0.0024409783103802314,\n",
       "  0.0024469264404122933,\n",
       "  0.0024481602285784596,\n",
       "  0.002450841127935272,\n",
       "  0.0024518071196855335,\n",
       "  0.0024516338385569118,\n",
       "  0.002451554366808996,\n",
       "  0.002451589031851516,\n",
       "  0.002452031488293213]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgtrain = lgb.Dataset(train, label=y_train.values.ravel())\n",
    "lgb.cv(params_lgb, xgtrain, 10, nfold=5, metrics='auc', early_stopping_rounds=10,\n",
    "       # This is what I added\n",
    "        stratified=False)\n",
    "\n",
    "\n",
    "#model = lgb.LGBMClassifier(**params)\n",
    "#model.fit(train, y_train)\n",
    "#y_predict = model.predict_proba(test)\n",
    "#return model, y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_list = [lgb.LGBMClassifier(**params_lgb) for i in range(11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-d55e86e9e9b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnew_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnew_y_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcls_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/anaconda3/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    693\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    696\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/anaconda3/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    472\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    202\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1526\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1527\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1528\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1529\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, idx in enumerate(idx_list):\n",
    "    new_train = train[idx, :]\n",
    "    new_y_train = y_train.values.ravel()[idx]\n",
    "    cls_list[i].fit(new_train, new_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_list = []\n",
    "for cls in cls_list:\n",
    "    predict_list.append(cls.predict_proba(test)[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.mean(np.array(predict_list), axis=0)\n",
    "\n",
    "output_result(df_test.index, res, sid='11models_mean_lgb_gridsearch_lhy')  # 0.772 11models_mean_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb = {\n",
    "    'nthread': 4,\n",
    "    #is_unbalance=True,\n",
    "    'n_estimators' : 100,\n",
    "    'learning_rate' : 0.1171,\n",
    "    'num_leaves' : 100,\n",
    "    'colsample_bytree' : 0.9604,\n",
    "    'subsample' : 0.9609,\n",
    "    'max_depth' : 7,\n",
    "    'reg_alpha' : 9.6523,\n",
    "    'reg_lambda' : 1,\n",
    "    'min_split_gain' : 0.179,\n",
    "    'min_child_weight' : 13,\n",
    "    'metric': 'auc',\n",
    "    #'silent': -1,\n",
    "    'verbose': -1,\n",
    "    #scale_pos_weight=11\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_list = [lgb.LGBMClassifier(**params_lgb) for i in range(11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, idx in enumerate(idx_list):\n",
    "    new_train = train[idx, :]\n",
    "    new_y_train = y_train.values.ravel()[idx]\n",
    "    cls_list[i].fit(new_train, new_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_list = []\n",
    "for cls in cls_list:\n",
    "    predict_list.append(cls.predict_proba(test)[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.mean(np.array(predict_list), axis=0)\n",
    "\n",
    "output_result(df_test.index, res, sid='11models_mean_lgb_change_leaf_num_100')  # 0.772"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试 RobusxtScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
      "0        0.705973       0.001680       0.700744      0.004082\n",
      "1        0.714776       0.002003       0.708041      0.005586\n",
      "2        0.721093       0.001433       0.712055      0.005532\n",
      "3        0.729924       0.001927       0.719409      0.005051\n",
      "4        0.733852       0.002252       0.722567      0.004702\n",
      "5        0.738533       0.001697       0.726365      0.004990\n",
      "6        0.742360       0.001394       0.729371      0.005238\n",
      "7        0.745623       0.001581       0.731764      0.005240\n",
      "8        0.749460       0.001838       0.734793      0.005099\n",
      "9        0.752700       0.001473       0.737192      0.005509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(XGBClassifier(alpha=9.6523, base_score=0.5, booster='gbtree',\n",
       "        colsample_bylevel=1, colsample_bytree=1, cosample_bytree=0.9604,\n",
       "        eta=0.1171, eval_metric='auc', gamma=0.179, learning_rate=0.1,\n",
       "        max_delta_step=0, max_depth=7, min_child_weight=13, missing=None,\n",
       "        n_estimators=100, n_jobs=1, nthread=4, objective='binary:logistic',\n",
       "        random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "        seed=918, silent=1, subsample=0.9609, verbose_eval=True),\n",
       " array([[0.9400627 , 0.05993729],\n",
       "        [0.8798291 , 0.12017091],\n",
       "        [0.9652743 , 0.03472573],\n",
       "        ...,\n",
       "        [0.9862132 , 0.01378676],\n",
       "        [0.9527332 , 0.04726679],\n",
       "        [0.8011434 , 0.19885659]], dtype=float32))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_no_feature_select(train, test, y_train, cv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = rs.fit_transform(train, y_train)\n",
    "test2 = rs.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
      "0        0.705973       0.001680       0.700744      0.004082\n",
      "1        0.714776       0.002003       0.708041      0.005586\n",
      "2        0.721093       0.001433       0.712055      0.005532\n",
      "3        0.729924       0.001927       0.719408      0.005051\n",
      "4        0.733852       0.002252       0.722566      0.004702\n",
      "5        0.738533       0.001697       0.726364      0.004990\n",
      "6        0.742360       0.001394       0.729370      0.005237\n",
      "7        0.745623       0.001581       0.731763      0.005240\n",
      "8        0.749460       0.001838       0.734792      0.005098\n",
      "9        0.752701       0.001473       0.737192      0.005509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(XGBClassifier(alpha=9.6523, base_score=0.5, booster='gbtree',\n",
       "        colsample_bylevel=1, colsample_bytree=1, cosample_bytree=0.9604,\n",
       "        eta=0.1171, eval_metric='auc', gamma=0.179, learning_rate=0.1,\n",
       "        max_delta_step=0, max_depth=7, min_child_weight=13, missing=None,\n",
       "        n_estimators=100, n_jobs=1, nthread=4, objective='binary:logistic',\n",
       "        random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "        seed=918, silent=1, subsample=0.9609, verbose_eval=True),\n",
       " array([[0.9540579 , 0.04594211],\n",
       "        [0.88835865, 0.11164133],\n",
       "        [0.96245784, 0.03754215],\n",
       "        ...,\n",
       "        [0.9888067 , 0.01119326],\n",
       "        [0.96069896, 0.03930106],\n",
       "        [0.79613644, 0.20386358]], dtype=float32))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_no_feature_select(train2, test2, y_train, cv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
